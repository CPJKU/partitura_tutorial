{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "c04b411e",
   "metadata": {},
   "source": [
    "# Symbolic Music Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64631779",
   "metadata": {},
   "source": [
    "Automatic Music Alignment refers to the task of linking or matching two musical signals of the same musical work. This can be, e.g., matching *different performances* of the same piece, or matching the performance of a piece with its musical score.\n",
    "\n",
    "The following figure shows a common music alignment pipeline:\n",
    "\n",
    "<img src=\"figures/alignment_pipeline.png\" alt=\"alignment_pipeline\" width=\"600\"/>\n",
    "\n",
    "In this part of the tutorial we are going to explore these components in more detail."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fe7b7",
   "metadata": {},
   "source": [
    "## Music Representation\n",
    "\n",
    "Music representations, since this is a tutorial on symbolic music processing, we will focus on symbolic music representations, such that can be stored in formats such as MIDI, MusicXML or MEI, and that can be generated by editors like MuseScore, Finale, etc.\n",
    "\n",
    "### Audio vs. Symbolic Alignment\n",
    "\n",
    "* In **Audio-to-audio alignment**, the alignment itself typically refers to  of *timestamps* (in absolute time in seconds) in one audio recording of a musical work to the corresponding *timestamp* in another recording. (In audio recordings, identifying individual notes is [not a trivial task!](ADD_LINK_TO_REFERENCES))\n",
    "\n",
    "* In **Symbolic-to-symbolic alignment**, we can have two types of alignment:\n",
    "    * **Time-wise alignments**: similar to audio-based alignment, we can map timestamps (in symbolic time units like musical beats or MIDI ticks) from one version of the work to another (e.g., a MIDI performance to a score in MusicXML/MEI/Humdrum format). \n",
    "    * **Note-wise alignment**: We can map individual symbolic music elements (most commonly notes) from one version to another. This is very useful for modeling expressive performance.\n",
    "\n",
    "\n",
    "### Types of music alignment\n",
    "\n",
    "We can categorize musical alignment in two main dimensions: (representation) modality and time.\n",
    "\n",
    "#### Representation modality\n",
    "\n",
    "\n",
    "* **Audio-to-audio alignment**: Alignment of two (audio) recordings. This is probably the most studied type of alignment in the MIR literature.\n",
    "* **Symbolic-to-audio alignment**: Alignment of symbolically encoded score events with timestamps positions in a recording.\n",
    "* **Image-to-audio alignment**: Alignment of spatial positions (e.g., bounding boxes of musical measures given in pixels) of digitized images of sheet music with time positions of a recording.\n",
    "* **Lyrics-to-audio alignment**: Alignment of lyrics (given in text format) with time positions of a recorded song.\n",
    "\n",
    "#### Time\n",
    "\n",
    "* **Offline**: Alignment of two *recordings/documents* (i.e., audio recordings, MIDI performances, MusicXML scores, etc.). These recordings/documents can be in any of the modalities described above, the important thing being that the music is occurring in real-time.\n",
    "\n",
    "* **Online**: Alignment of a live (i.e., real time) performance to the music encoded in a target document (e.g., a pre-annotated audio recording, a symbolic score, etc.). The problem of real time online alignment is known in the MIR literature a **score following**, and can be useful in live interactive settings, such as automatic accompaniment systems\n",
    "\n",
    "In this tutorial we are going to focus on the case of offline alignment (but there will be a real-time demo at the end 😉)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b323c",
   "metadata": {},
   "source": [
    "## Feature Representations\n",
    "\n",
    "To make musical data comparable for alignment algorithms, the first step is to extract features that capture relevant aspects while suppressing irrelevant details.\n",
    "\n",
    "Let's make a quick mathematical parenthesis. For algorithmic purposes, it is convenient to represent the music captured by whichever music representation that we working with as *sequences of features*. \n",
    "\n",
    "Let us consider two sequences $\\mathbf{X} = \\{\\mathbf{x}_1, \\dots \\mathbf{x}_N\\}$ and $\\mathbf{Y} = \\{\\mathbf{y}_1, \\dots, \\mathbf{y}_M\\}$ for which we want to find an aligment.\n",
    "\n",
    "* This sequences could be discrete signals, feature sequences, sequences of characters, etc.\n",
    "\n",
    "* The elements $\\mathbf{x}_n$, $\\mathbf{y}_m$ belong to the same **feature space** $\\mathcal{F}$. For the purposes of this tutorial, let us consider these elements as $K$-dimensional real-vectors, i.e., $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^K$ although they can other kind of objects (e.g., characters in an alphabet).\n",
    "\n",
    "An important aspect of this feature space is that it allows us to use *quantitive measures* of how similar the elements of sequence $\\mathbf{X}$ are to the elements in sequence $\\mathbf{Y}$. We will come back to this point in a moment.\n",
    "\n",
    "In this tutorial we are going to focus on 2 common features representations:\n",
    "\n",
    "1. Piano Rolls\n",
    "2. Pitch Class Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc0f43",
   "metadata": {},
   "source": [
    "### Piano Rolls\n",
    "\n",
    "A piano roll is a 2D representation of (MIDI) pitch and time. We can extract piano rolls from symbolic music files with Partitura!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing some stuff\n",
    "import os\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import partitura as pt\n",
    "\n",
    "from helper import (\n",
    "    compute_pitch_class_pianoroll, \n",
    "    generate_example_sequences, \n",
    "    plot_alignment\n",
    ")\n",
    "\n",
    "from typing import List\n",
    "\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a score and a performance of the score\n",
    "score_fn = \"content/mozart_k265_var1.musicxml\"\n",
    "performance_fn = \"content/mozart_k265_var1.mid\"\n",
    "\n",
    "score = pt.load_score(score_fn)\n",
    "performance = pt.load_performance(performance_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-wagon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute piano roll\n",
    "use_piano_range = False\n",
    "score_pr = pt.utils.music.compute_pianoroll(\n",
    "    note_info=score,\n",
    "    # time_unit=\"auto\"\n",
    "    # time_div=\"auto\",\n",
    "    # onset_only=False,\n",
    "    # note_separation=False,\n",
    "    # remove_silence=True,\n",
    "    piano_range=use_piano_range,\n",
    "    # return_idxs=False,\n",
    ")\n",
    "\n",
    "performance_pr = pt.utils.music.compute_pianoroll(\n",
    "    note_info=performance,\n",
    "    # time_unit=\"auto\"\n",
    "    # time_div=\"auto\",\n",
    "    # onset_only=False,\n",
    "    # note_separation=False,\n",
    "    # remove_silence=True,\n",
    "    piano_range=use_piano_range,\n",
    "    # return_idxs=False,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420be1b4",
   "metadata": {},
   "source": [
    "Let's have a look at the output of these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-enemy",
   "metadata": {},
   "source": [
    "By default, piano rolls computed with partitura are stored in scipy's sparse matrices, since most of the elements are 0.\n",
    "\n",
    "The first dimension of the array is MIDI pitch (128) and the second dimension are discrete time-steps defined by the `time_div` and `time_unit` arguments of the  `compute_pianoroll` function.\n",
    "\n",
    "Let's visualize the piano rolls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize=(10, 7))\n",
    "axes[0].imshow(\n",
    "    score_pr.todense(), \n",
    "    aspect = \"auto\", \n",
    "    origin=\"lower\", \n",
    "    cmap=\"gray\",\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "axes[1].imshow(\n",
    "    performance_pr.todense(), \n",
    "    aspect = \"auto\", \n",
    "    origin=\"lower\", \n",
    "    cmap=\"gray\",\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "y_label = \"Piano key\" if use_piano_range else \"MIDI pitch\"\n",
    "axes[0].set_ylabel(y_label)\n",
    "axes[1].set_ylabel(y_label)\n",
    "axes[1].set_xlabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-addition",
   "metadata": {},
   "source": [
    "For more information, see the documentation of  [`compute_pianoroll`](https://partitura.readthedocs.io/en/latest/modules/partitura.utils.html#partitura.utils.compute_pianoroll)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-oxford",
   "metadata": {},
   "source": [
    "### Pitch Class Distributions\n",
    "\n",
    "These features are the symbolic equivalent to *chroma* features in audio. This representation is basically a piano roll that has been folded into a single octave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pc_pr = compute_pitch_class_pianoroll(\n",
    "    score,\n",
    "    normalize=True,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-mediterranean",
   "metadata": {},
   "source": [
    "Let's plot this feature and compare it to a piano roll of the same score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pr = pt.utils.music.compute_pianoroll(\n",
    "    note_info=score,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=4,\n",
    "    piano_range=False\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize=(10, 5), sharex=True)\n",
    "\n",
    "axes[0].imshow(score_pc_pr, aspect = \"auto\", \n",
    "    origin=\"lower\", \n",
    "    cmap=\"gray\",\n",
    "    interpolation=\"nearest\",)\n",
    "axes[1].imshow(score_pr.todense(), aspect=\"auto\", origin=\"lower\", cmap=\"gray\", interpolation=\"nearest\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d8d3c",
   "metadata": {},
   "source": [
    "## Alignment Methods\n",
    "\n",
    "We move now to methods for computing the alignment between features from one version of a piece of music to another.\n",
    "\n",
    "Common methods are dynamic programming approaches like dynamic time warping (the focus of this tutorial) and probabilistic approaches like hidden Markov models. In this tutorial we are going to show how to perform alignment using DTW. \n",
    "\n",
    "Dynamic time warping (DTW) is a dynamic programming algorithm to find the **optimal** alignment between to time-dependent sequences. In a nutshell, the DTW algorithm finds the alignment between two sequence in three steps:\n",
    "\n",
    "1. Compute the pairwise distance between elements in sequence $\\mathbf{X}$ and $\\mathbf{Y}$.\n",
    "2. Compute the accumulated cost\n",
    "3. Find the best alignment by backtracking \n",
    "\n",
    "For now, let us generate some test data that we can play with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper method to generate sample sequences \n",
    "# (see helper.py for documentation)\n",
    "\n",
    "# lenght of the \"reference\" sequence\n",
    "lenX = 15\n",
    "\n",
    "# dimensionality of the feature space\n",
    "K = 5\n",
    "\n",
    "# This method generates an example sequence\n",
    "X, Y, gr_path = generate_example_sequences(\n",
    "    lenX=lenX, \n",
    "    centers=3, \n",
    "    n_features=K,\n",
    "    maxreps=4, \n",
    "    minreps=1, \n",
    "    noise_scale=0.1\n",
    ")\n",
    "\n",
    "# Let us plot the data to see how it looks like!\n",
    "plot_alignment(X, Y, gr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-briefing",
   "metadata": {},
   "source": [
    "### Comparing the similarity of the features: Local cost distance\n",
    "\n",
    "We would like to know how to compare the elements in $\\mathbf{X}$ and $\\mathbf{Y}$. For this we use a local distance function, which can be any distance(-like) function that is small when $\\mathbf{x}_i$ is *similar* to $\\mathbf{y}_j$.\n",
    "\n",
    "Which distance to use depends on the problem at hand, although usual starting points are the Euclidean and the Manhattan ($L_1$) distances.\n",
    "\n",
    "Using this local distance, we can compare the elements in both sequences by comparing the pairwise distance of all elements in $\\mathbf{X}$ and $\\mathbf{Y}$. This will result in a matrix $\\mathbf{C}$, where the element $\\mathbf{C}[i,j]$ is given by\n",
    "\n",
    "$$\\mathbf{C}[i,j] = \\text{distance}(\\mathbf{x}_i, \\mathbf{y}_j)$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "underlying-conclusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.spatial.distance import cdist\n",
    "\n",
    "def pairwise_distance_matrix(\n",
    "    X: np.ndarray, \n",
    "    Y: np.ndarray, \n",
    "    metric: str = 'euclidean'\n",
    ") -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Compute pairwise distance matrix of two sequences\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        A 2D array with size (n_observations, n_features)\n",
    "    Y : np.ndarray\n",
    "        A 2D array with size (m_observations, n_features)\n",
    "    metric: str\n",
    "        A string defining a metric (see possibilities \n",
    "        in scipy.spatial.distance.cdist)\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    C : np.ndarray\n",
    "        Pairwise cost matrix\n",
    "    \"\"\"\n",
    "    if X.ndim == 1:\n",
    "        X, Y = np.atleast_2d(X, Y)\n",
    "        X = X.T\n",
    "        Y = Y.T\n",
    "    C = cdist(X, Y, metric=metric)\n",
    "    return C"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-slovenia",
   "metadata": {},
   "source": [
    "Let's visualize the pairwise cost matrix"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Visualize cost matrix\n",
    "C = pairwise_distance_matrix(X, Y)\n",
    "plt.imshow(C, origin='lower', aspect='equal', cmap='gray')\n",
    "plt.xlabel(r'$\\mathbf{Y}$')\n",
    "plt.ylabel(r'$\\mathbf{X}$')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-poster",
   "metadata": {},
   "source": [
    "The DTW Algorithm\n",
    "\n",
    "* **Input**: Cost matrix $\\mathbf{C}$ of size $N \\times M$\n",
    "* **Output**: Optimal warping path $P^*$, $d_{DTW}$\n",
    "\n",
    "**Procedure**\n",
    "\n",
    "1. Initialize $N \\times M$ matrix $D$ (accumulated cost) by\n",
    "\n",
    "$$\\mathbf{D}[n, 0] = \\sum_{k=0}^{n} \\mathbf{C}[k, 0]$$\n",
    "\n",
    "for $n \\in [0,N-1]$, and\n",
    "\n",
    "$$\\mathbf{D}[0, m] = \\sum_{k=0}^{m} \\mathbf{C}[0, k]$$\n",
    "\n",
    "for $n \\in [0, M-1]$\n",
    "\n",
    "2. Compute in a nested loop for $n=1,\\dots, N-1$ and $m=1, \\dots, M-1$\n",
    "\n",
    "$$\\mathbf{D}[n, m] = \\mathbf{C}[n, m] + \\min \\left\\{\\mathbf{D}[n-1, m-1], \\mathbf{D}[n-1, m], \\mathbf{D}[n, m-1] \\right\\}$$\n",
    "\n",
    "3. Get the warping path. Set $l = 0$ and $q_0 = (N-1, M-1)$. Repeat the following steps until $q_l = (0, 0)$\n",
    "    1. $l \\leftarrow l+1$ and $(n, m) = q_{l -1}$\n",
    "    2. If $n = 0$ then $q_l = (0, m - 1)$,\n",
    "    3. else if \n",
    "\n",
    "4. The dynamic time warping distance is given by\n",
    "\n",
    "$$d_{DTW}(\\mathbf{X}, \\mathbf{Y}) = D[N-1, M-1]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "def accumulated_cost_matrix(C: np.ndarray) -> np.ndarray:\n",
    "    \"\"\"\n",
    "    Dynamic time warping cost matrix from a pairwise distance matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    D : double array\n",
    "        Pairwise distance matrix (computed e.g., with `cdist`).\n",
    "\n",
    "    Returns\n",
    "    -------\n",
    "    D : np.ndarray\n",
    "        Accumulated cost matrix\n",
    "    \"\"\"\n",
    "    N = C.shape[0]\n",
    "    M = C.shape[1]\n",
    "    D = np.zeros((N, M))\n",
    "    D[0, 0] = C[0, 0]\n",
    "    for n in range(1, N):\n",
    "        D[n, 0] = D[n-1, 0] + C[n, 0]\n",
    "    for m in range(1, M):\n",
    "        D[0, m] = D[0, m-1] + C[0, m]\n",
    "    for n in range(1, N):\n",
    "        for m in range(1, M):\n",
    "            D[n, m] = C[n, m] + min(D[n-1, m], D[n, m-1], D[n-1, m-1])\n",
    "    return D\n",
    "\n",
    "D = accumulated_cost_matrix(C) \n",
    "\n",
    "# Visualize accumulated cost matrix\n",
    "plt.imshow(D, origin='lower', aspect='equal', cmap='gray')\n",
    "plt.xlabel(r'Sequence $\\mathbf{Y}$')\n",
    "plt.ylabel(r'Sequence $\\mathbf{X}$')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "timely-andrews",
   "metadata": {},
   "outputs": [],
   "source": [
    "def optimal_warping_path(D):\n",
    "    \"\"\"\n",
    "    Compute the warping path given an accumulated cost matrix\n",
    "\n",
    "    Parameters\n",
    "    ----------\n",
    "    D: np.ndarray\n",
    "        Accumulated cost Matrix\n",
    "    \n",
    "    Returns\n",
    "    -------\n",
    "    P: np.ndarray\n",
    "        Optimal warping path\n",
    "    \"\"\"\n",
    "    N = D.shape[0]\n",
    "    M = D.shape[1]\n",
    "    n = N - 1\n",
    "    m = M - 1\n",
    "    P = [(n, m)]\n",
    "    while n > 0 or m > 0:\n",
    "        if n == 0:\n",
    "            cell = (0, m - 1)\n",
    "        elif m == 0:\n",
    "            cell = (n - 1, 0)\n",
    "        else:\n",
    "            val = min(D[n-1, m-1], D[n-1, m], D[n, m-1])\n",
    "            if val == D[n-1, m-1]:\n",
    "                cell = (n-1, m-1)\n",
    "            elif val == D[n-1, m]:\n",
    "                cell = (n-1, m)\n",
    "            else:\n",
    "                cell = (n, m-1)\n",
    "        P.append(cell)\n",
    "        (n, m) = cell\n",
    "    P.reverse()\n",
    "    return np.array(P)\n",
    "        \n",
    "P = optimal_warping_path(D)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(9, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(C, cmap='gray_r', origin='lower', aspect='equal')\n",
    "plt.plot(P[:, 1], P[:, 0], marker='o', color='r')\n",
    "plt.clim([0, np.max(C)])\n",
    "plt.colorbar()\n",
    "plt.title('$C$ with optimal warping path')\n",
    "plt.xlabel('Sequence Y')\n",
    "plt.ylabel('Sequence X')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(D, cmap='gray_r', origin='lower', aspect='equal')\n",
    "plt.plot(P[:, 1], P[:, 0], marker='o', color='r')\n",
    "plt.plot(gr_path[:, 1], gr_path[:, 0], marker='d', color='purple', linewidth=1.1)\n",
    "plt.clim([0, np.max(D)])\n",
    "plt.colorbar()\n",
    "plt.title('$D$ with optimal warping path')\n",
    "plt.xlabel('Sequence Y')\n",
    "plt.ylabel('Sequence X')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "overhead-masters",
   "metadata": {},
   "source": [
    "Let's put everything together into a single function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "proper-phrase",
   "metadata": {},
   "outputs": [],
   "source": [
    "def naive_dynamic_time_warping(\n",
    "    X: np.ndarray, \n",
    "    Y: np.ndarray, \n",
    "    metric: str ='euclidean',\n",
    "    return_distance: bool = False,\n",
    ") -> (np.ndarray, float):\n",
    "    \"\"\"\n",
    "    Naive Implementation of Vanilla Dynamic Time Warping\n",
    "    \n",
    "    Parameters\n",
    "    ----------\n",
    "    X : np.ndarray\n",
    "        Array X\n",
    "    Y: np.ndarray\n",
    "        Array Y\n",
    "    metric: string\n",
    "        Scipy Metric\n",
    "        \n",
    "    Returns\n",
    "    -------\n",
    "    warping_path: np.ndarray\n",
    "        The warping path for the optimal alignment.\n",
    "    dtwd : float\n",
    "        The dynamic time warping distance of the alignment.\n",
    "    \"\"\"\n",
    "    # Compute pairwise distance matrix\n",
    "    C = pairwise_distance_matrix(X, Y, metric=metric)\n",
    "    # Compute accumulated cost matrix\n",
    "    D = accumulated_cost_matrix(C)\n",
    "    dtwd = D[-1, -1]\n",
    "    # Get warping path\n",
    "    warping_path = optimal_warping_path(D)\n",
    "    \n",
    "    if return_distance:\n",
    "        return warping_path, dtwd\n",
    "    return warping_path"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-webster",
   "metadata": {},
   "source": [
    "This naive implementation is very slow! You can only use it for aligning small sequences. For practical stuff, we are going to use the `fasdtw` package. This package contains an efficient implementation of vanilla DTW, as well as a faster approximation, called FastDTW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The alignment.py module includes\n",
    "from alignment import dynamic_time_warping, fast_dynamic_time_warping\n",
    "import time\n",
    "\n",
    "for lenX in [10, 100, 1000]:\n",
    "    X, Y, gr_path = generate_example_sequences(\n",
    "        lenX=lenX, \n",
    "        centers=3, \n",
    "        n_features=K,\n",
    "        maxreps=2, \n",
    "        minreps=2, \n",
    "        noise_scale=0.1\n",
    "    )\n",
    "    st = time.process_time()\n",
    "    path_naive, dtwd_naive = naive_dynamic_time_warping(X, Y, return_distance=True)\n",
    "    et_naive = time.process_time() - st\n",
    "\n",
    "    st = time.process_time()\n",
    "    path_optimized, dtwd_optimized = dynamic_time_warping(X, Y, return_distance=True)\n",
    "    et_optimized = time.process_time() - st\n",
    "\n",
    "    st = time.process_time()\n",
    "    path_fdtw, dtwd_fdtw = fast_dynamic_time_warping(X, Y, return_distance=True)\n",
    "    et_fdtw = time.process_time() - st\n",
    "\n",
    "    print(f\"Dynamic Time Warping Distance {lenX} {len(Y)}\")\n",
    "    print(f\"Naive: {dtwd_naive:.3f} ({et_naive * 1000:.2f} ms)\") \n",
    "    print(f\"Optimized: {dtwd_optimized:.3f} ({et_optimized * 1000:.2f} ms)\")\n",
    "    print(f\"FastDTW: {dtwd_fdtw:.3f} ({et_fdtw * 1000:.2f} ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-allergy",
   "metadata": {},
   "source": [
    "### Creating note-level alignments from sequential alignment information\n",
    "\n",
    "Dynamic Time Warping and related sequence alignment algorithms return a path between two sequences or time series. Note alignment of two polyphonic parts is categorically different from a time series alignment. To get to a note alignment, we need to figure out what notes are played at a specific time in the piano roll. Sometimes this information might be imprecise so we need to relax the search for notes at some piano roll time to find all relevant notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pr, sidx = pt.utils.music.compute_pianoroll(\n",
    "    note_info=score,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=8,\n",
    "    return_idxs=True,\n",
    "    piano_range=True,\n",
    ")\n",
    "\n",
    "performance_pr, pidx = pt.utils.music.compute_pianoroll(\n",
    "    note_info=performance,\n",
    "    time_unit=\"sec\",\n",
    "    time_div=10,\n",
    "    return_idxs=True,\n",
    "    piano_range=True\n",
    ")\n",
    "\n",
    "reference_features = score_pr.todense().T\n",
    "performance_features = performance_pr.todense().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pitch_index, onset, offset, midi_pitch\n",
    "sidx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx correspond to notes in note_array\n",
    "snote_array = score.note_array()\n",
    "print(snote_array[:5])\n",
    "\n",
    "# Check that the pitch in the note array corresponds to\n",
    "# the fourth column in the indices from the note array\n",
    "assert(all(snote_array[\"pitch\"] == sidx[:, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic time warping\n",
    "dtw_alignment, _ = fast_dynamic_time_warping(\n",
    "    X=reference_features, \n",
    "    Y=performance_features, \n",
    "    metric=\"cosine\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "monthly-termination",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import greedy_note_alignment\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_alignment = greedy_note_alignment(dtw_alignment, sidx, score.note_array(), pidx, performance.note_array())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2ac9e",
   "metadata": {},
   "source": [
    "## Comparing alignments\n",
    "\n",
    "Let's compare different alignment methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d4d3ec3",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains the ground truth alignment\n",
    "gt_alignment_fn = \"content/mozart_k265_var1.match\"\n",
    "\n",
    "al_perf, gt_alignment = pt.load_match(gt_alignment_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "992d4781",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5973da44",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invent a linear alignment for testing\n",
    "from helper import dummy_linear_alignment\n",
    "\n",
    "# Dummy linear alignment\n",
    "linear_alignment = dummy_linear_alignment(\n",
    "    X=reference_features, \n",
    "    Y=performance_features,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(linear_alignment[:, 0], linear_alignment[:, 1], label=\"linear\")\n",
    "plt.plot(dtw_alignment[:, 0], dtw_alignment[:, 1], label=\"DTW (piano roll)\")\n",
    "plt.legend()\n",
    "plt.xlabel('Index in score')\n",
    "plt.ylabel('Index in performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "inside-spending",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "initial-mobile",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "wireless-projection",
   "metadata": {},
   "source": [
    "To inspect an alignment, we can use [**Parangonada**](https://sildater.github.io/parangonada/), a tool to compare alignments developed at our institute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export files to Parangonada\n",
    "outdir = \"parangonada_files\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "pt.save_parangonada_csv(\n",
    "    alignment=note_alignment,\n",
    "    performance_data=performance,\n",
    "    score_data=score,\n",
    "    outdir=\"parangonada_files\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-integral",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import evaluate_alignment_notewise\n",
    "\n",
    "precision, recall, fscore = evaluate_alignment_notewise(\n",
    "    prediction=note_alignment,\n",
    "    ground_truth=gt_alignment\n",
    ")\n",
    "\n",
    "print(precision, recall, fscore)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e74fea82",
   "metadata": {},
   "source": [
    "## Conclusions\n",
    "\n",
    "In this notebook we explored symbolic alignment with DTW. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650c3a0",
   "metadata": {},
   "source": [
    "## References\n",
    "\n",
    "#### Alignment Basics\n",
    "\n",
    "* M. Müller.\n",
    "\n",
    "* A. Arzt.\n",
    "\n",
    "#### DTW\n",
    "\n",
    "#### HMMs\n",
    "\n",
    "#### Lyrics to audio alignment\n",
    "\n",
    "* N. Lizé-Masclef, A. Vaglio, M. Moussallam. “User-centered evaluation of lyrics to audio alignment”, International Society for Music Information Retrieval (ISMIR) conference, 2021.\n",
    "\n",
    "* M. Mauch, F: Hiromasa, M. Goto. “Lyrics-to-audio alignment and phrase-level segmentation using incomplete internet-style chord annotations”, Frontiers in Proceedings of the Sound Music Computing Conference (SMC), 2010.\n",
    "\n",
    "* G. Dzhambazov. “Knowledge-Based Probabilistic Modeling For Tracking Lyrics In Music Audio Signals”, PhD Thesis, 2017.\n",
    "\n",
    "* H. Fujihara, M. Goto, J. Ogata, H. Okuno. “LyricSynchronizer: Automatic synchronization system between musical audio signals and lyrics”, IEEE Journal of Selected Topics in Signal Processing, VOL. 5, NO. 6, 2011"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
