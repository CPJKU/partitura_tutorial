{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "9eed8832",
   "metadata": {},
   "source": [
    "[![Open In Colab](https://colab.research.google.com/assets/colab-badge.svg)](https://colab.research.google.com/github/CPJKU/partitura_tutorial/blob/alignment/content/02_alignment/Symbolic_Music_Alignment.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c04b411e",
   "metadata": {},
   "source": [
    "# 2. Symbolic Music Alignment"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "64631779",
   "metadata": {},
   "source": [
    "Automatic Music Alignment refers to the task of linking or matching two musical signals of the same musical work. This can be, e.g., matching *different performances* of the same piece, or matching the performance of a piece with its musical score.\n",
    "\n",
    "The following figure shows a common music alignment pipeline:\n",
    "\n",
    "<img src=\"figures/alignment_pipeline.png\" alt=\"alignment_pipeline\" width=\"600\"/>\n",
    "\n",
    "In this part of the tutorial we are going to explore these components in more detail."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "53bee8c7",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    import google.colab\n",
    "    IN_COLAB = True\n",
    "except:\n",
    "    IN_COLAB = False\n",
    "\n",
    "if IN_COLAB:\n",
    "    # Install partitura\n",
    "    ! pip install git+https://github.com/CPJKU/partitura.git@pitch_class_pr\n",
    "    ! pip install fastdtw\n",
    "    \n",
    "    # To be able to access helper modules in the repo for this tutorial\n",
    "    # (not necessary if the jupyter notebook is run locally instead of google colab)\n",
    "    !git clone -b alignment https://github.com/CPJKU/partitura_tutorial.git\n",
    "    !git clone https://github.com/OFAI/vienna4x22_rematched.git\n",
    "    \n",
    "    import sys\n",
    "    sys.path.insert(0,'/content/partitura_tutorial/content/02_alignment')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6d0bb708",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "\n",
    "if IN_COLAB:\n",
    "    V4X22_DATASET_DIR = '/content/vienna4x22_rematched'\n",
    "else:\n",
    "    # Path to the Vienna 4x22 dataset\n",
    "    V4X22_DATASET_DIR = '/Users/carlos/Repos/vienna4x22/'\n",
    "\n",
    "MUSICXML_DIR = os.path.join(V4X22_DATASET_DIR, 'musicxml')\n",
    "MIDI_DIR = os.path.join(V4X22_DATASET_DIR, 'midi')\n",
    "MATCH_DIR = os.path.join(V4X22_DATASET_DIR, 'match')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb6fe7b7",
   "metadata": {},
   "source": [
    "## 2.1. Music Representation\n",
    "\n",
    "Music representations, since this is a tutorial on symbolic music processing, we will focus on symbolic music representations, such that can be stored in formats such as MIDI, MusicXML or MEI, and that can be generated by editors like MuseScore, Finale, etc.\n",
    "\n",
    "### 2.1.1. Audio vs. Symbolic Alignment\n",
    "\n",
    "* In **Audio-to-audio alignment**, the alignment itself typically refers to  of *timestamps* (in absolute time in seconds) in one audio recording of a musical work to the corresponding *timestamp* in another recording. (In audio recordings, identifying individual notes is [not a trivial task!](ADD_LINK_TO_REFERENCES))\n",
    "\n",
    "* In **Symbolic-to-symbolic alignment**, we can have two types of alignment:\n",
    "    * **Time-wise alignments**: similar to audio-based alignment, we can map timestamps (in symbolic time units like musical beats or MIDI ticks) from one version of the work to another (e.g., a MIDI performance to a score in MusicXML/MEI/Humdrum format). \n",
    "    * **Note-wise alignment**: We can map individual symbolic music elements (most commonly notes) from one version to another. This is very useful for modeling expressive performance.\n",
    "\n",
    "\n",
    "### 2.1.2. Types of music alignment\n",
    "\n",
    "We can categorize musical alignment in two main dimensions: (representation) modality and time.\n",
    "\n",
    "#### 2.1.2.1. Representation modality\n",
    "\n",
    "\n",
    "* **Audio-to-audio alignment**: Alignment of two (audio) recordings. This is probably the most studied type of alignment in the MIR literature.\n",
    "* **Symbolic-to-audio alignment**: Alignment of symbolically encoded score events with timestamps positions in a recording.\n",
    "* **Image-to-audio alignment**: Alignment of spatial positions (e.g., bounding boxes of musical measures given in pixels) of digitized images of sheet music with time positions of a recording.\n",
    "* **Lyrics-to-audio alignment**: Alignment of lyrics (given in text format) with time positions of a recorded song.\n",
    "\n",
    "#### 2.1.2.2. Time\n",
    "\n",
    "* **Offline**: Alignment of two *recordings/documents* (i.e., audio recordings, MIDI performances, MusicXML scores, etc.). These recordings/documents can be in any of the modalities described above, the important thing being that the music is occurring in real-time.\n",
    "\n",
    "* **Online**: Alignment of a live (i.e., real time) performance to the music encoded in a target document (e.g., a pre-annotated audio recording, a symbolic score, etc.). The problem of real time online alignment is known in the MIR literature a **score following**, and can be useful in live interactive settings, such as automatic accompaniment systems\n",
    "\n",
    "In this tutorial we are going to focus on the case of offline alignment (but there will be a real-time demo at the end 😉)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "floating-madison",
   "metadata": {},
   "source": [
    "### 2.2. Handling Alignment Information (Match files)\n",
    "\n",
    "#### 2.2.1. Loading Alignments\n",
    "An important use case of partitura is to handle symbolic alignment information\n",
    "\n",
    "**Note that partitura itself does not contain methods for alignment**\n",
    "\n",
    "Partitura supports 2 formats for encoding score-to-performance alignments\n",
    "\n",
    "* Our match file format, introduced by Gerhard et al. ;)\n",
    "    * Datasets including match files: Vienna4x22, ASAP, Batik (soon!)\n",
    "* The format introduced by [Nakamura et al. (2017)](https://eita-nakamura.github.io/articles/EN_etal_ErrorDetectionAndRealignment_ISMIR2017.pdf)\n",
    "\n",
    "Let's load an alignment!\n",
    "\n",
    "We have two common use cases\n",
    "\n",
    "* We have both the match file and the symbolic score file (e.g., MusicXML or MEI)\n",
    "* We have only the match file (only works for our format!)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "crucial-virus",
   "metadata": {},
   "source": [
    "##### 2.2.1.1. Loading an alignment if we only have a match file\n",
    "\n",
    "A useful property of match files is that they include information about the **score and the performance**. Therefore, it is possible to create both a  `Part` and a `PerformedPart` directly from a match file.\n",
    "\n",
    "* Match files contain all information included in performances in MIDI files, i.e., a MIDI file could be reconstructed from a match file.\n",
    "\n",
    "* Match files include all information information about pitch spelling and score position and duration of the notes in the score, as well as time and key signature information, and can encode some note-level markings, like accents. Nevertheless, it is important to note that the score information included in a match file is not necessarily complete. For example, match files do not generally include dynamics or tempo markings."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8f48042a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's start by importing some stuff\n",
    "import warnings\n",
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "import glob\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import partitura as pt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "rolled-cloud",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the match\n",
    "match_fn = os.path.join(MATCH_DIR, 'Chopin_op10_no3_p01.match')\n",
    "\n",
    "# loading a match file\n",
    "performance, alignment, score = pt.load_match(match_fn, create_score=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wooden-looking",
   "metadata": {},
   "source": [
    "##### 2.2.1.2. Loading an alignment if we have both score and match files\n",
    "\n",
    "In many cases, however, we have access to both the score and match files. Using the original score file has a few advantages:\n",
    "\n",
    "* It ensures that the score information is correct. Generating a `Part` from a match file involves inferring information for non-note elements (e.g., start and end time of the measures, voice information, clefs, staves, etc.).\n",
    "* If we want to load several performances of the same piece, we can load the score only once!\n",
    "\n",
    "This should be the preferred way to get alignment information!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "latest-smell",
   "metadata": {},
   "outputs": [],
   "source": [
    "# path to the match\n",
    "match_fn = os.path.join(MATCH_DIR, 'Chopin_op10_no3_p01.match')\n",
    "\n",
    "# Path to the MusicXML file\n",
    "score_fn = os.path.join(MUSICXML_DIR, 'Chopin_op10_no3.musicxml')\n",
    "\n",
    "# Load the score into a `Score` object\n",
    "score = pt.load_musicxml(score_fn)\n",
    "\n",
    "# loading a match file\n",
    "performance, alignment = pt.load_match(match_fn)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "pending-college",
   "metadata": {},
   "source": [
    "Score-to-performance alignments are represented by lists of dictionaries, which contain the following keys:\n",
    "\n",
    "* `label`\n",
    "\n",
    "    * `'match'`: there is a performed note corresponding to a score note\n",
    "    * `'insertion'`: the performed note does not correspond to any note in the score\n",
    "    * `'deletion'`: there is no performed note corresponding to a note in the score\n",
    "    * `'ornament'`: the performed note corresponds to the performance of an ornament (e.g., a trill). These notes are matched to the main note in the score. Not all alignments (in the datasets that we have) include ornamnets! Otherwise, ornaments are just treated as insertions.\n",
    "* `score_id`: id of the note in the score (in the `Part` object) (only relevant for matches, deletions and ornaments)\n",
    "* `performance_id`: Id of the note in the performance (in the `PerformedPart`) (only relevant for matches, insertions and ornaments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "radio-interim",
   "metadata": {},
   "outputs": [],
   "source": [
    "alignment[:10]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "exact-decrease",
   "metadata": {},
   "source": [
    "#### 2.1.2 Getting information from the alignments\n",
    "\n",
    "Partitura includes a few methods for getting information from the alignments.\n",
    "\n",
    "Let's start by getting the subset of score notes that have a corresponding performed note"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "published-understanding",
   "metadata": {},
   "outputs": [],
   "source": [
    "# note array of the score\n",
    "snote_array = score.note_array()\n",
    "# note array of the performance\n",
    "pnote_array = performance.note_array()\n",
    "# indices of the notes that have been matched\n",
    "matched_note_idxs = pt.utils.music.get_matched_notes(\n",
    "    spart_note_array=snote_array, \n",
    "    ppart_note_array=pnote_array, \n",
    "    alignment=alignment,\n",
    ")\n",
    "\n",
    "# note array of the matched score notes\n",
    "matched_snote_array = snote_array[matched_note_idxs[:, 0]]\n",
    "# note array of the matched performed notes\n",
    "matched_pnote_array = pnote_array[matched_note_idxs[:, 1]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b61b323c",
   "metadata": {},
   "source": [
    "## 2.2. Feature Representations\n",
    "\n",
    "To make musical data comparable for alignment algorithms, the first step is to extract features that capture relevant aspects while suppressing irrelevant details.\n",
    "\n",
    "Let's make a quick mathematical parenthesis. For algorithmic purposes, it is convenient to represent the music captured by whichever music representation that we working with as *sequences of features*. \n",
    "\n",
    "Let us consider two sequences $\\mathbf{X} = \\{\\mathbf{x}_1, \\dots \\mathbf{x}_N\\}$ and $\\mathbf{Y} = \\{\\mathbf{y}_1, \\dots, \\mathbf{y}_M\\}$ for which we want to find an aligment.\n",
    "\n",
    "* This sequences could be discrete signals, feature sequences, sequences of characters, etc.\n",
    "\n",
    "* The elements $\\mathbf{x}_n$, $\\mathbf{y}_m$ belong to the same **feature space** $\\mathcal{F}$. For the purposes of this tutorial, let us consider these elements as $K$-dimensional real-vectors, i.e., $\\mathbf{x}, \\mathbf{y} \\in \\mathbb{R}^K$ although they can other kind of objects (e.g., characters in an alphabet).\n",
    "\n",
    "An important aspect of this feature space is that it allows us to use *quantitive measures* of how similar the elements of sequence $\\mathbf{X}$ are to the elements in sequence $\\mathbf{Y}$. We will come back to this point in a moment.\n",
    "\n",
    "In this tutorial we are going to focus on 2 common features representations:\n",
    "\n",
    "1. Piano Rolls\n",
    "2. Pitch Class Distributions"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36cc0f43",
   "metadata": {},
   "source": [
    "### 2.2.1. Piano Rolls\n",
    "\n",
    "A piano roll is a 2D representation of (MIDI) pitch and time. We can extract piano rolls from symbolic music files with Partitura!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cf2ef0",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import (\n",
    "    compute_pitch_class_pianoroll, \n",
    "    generate_example_sequences, \n",
    "    plot_alignment\n",
    ")\n",
    "\n",
    "from typing import List\n",
    "\n",
    "%config InlineBackend.figure_format ='retina'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "individual-indonesian",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Let's load a score and a performance of the score\n",
    "\n",
    "# Path to the MusicXML file\n",
    "score_fn = os.path.join(MUSICXML_DIR, 'Chopin_op10_no3.musicxml')\n",
    "# score_fn = \"content/mozart_k265_var1.musicxml\"\n",
    "performance_fn = os.path.join(MIDI_DIR, 'Chopin_op10_no3_p01.mid')\n",
    "# performance_fn = \"content/mozart_k265_var1.mid\"\n",
    "\n",
    "score = pt.load_score(score_fn)\n",
    "performance = pt.load_performance(performance_fn)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "provincial-wagon",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# Compute piano roll\n",
    "use_piano_range = False\n",
    "score_pr = pt.utils.music.compute_pianoroll(\n",
    "    note_info=score,\n",
    "    # time_unit=\"auto\"\n",
    "    # time_div=\"auto\",\n",
    "    # onset_only=False,\n",
    "    # note_separation=False,\n",
    "    # remove_silence=True,\n",
    "    piano_range=use_piano_range,\n",
    "    # return_idxs=False,\n",
    ")\n",
    "\n",
    "performance_pr = pt.utils.music.compute_pianoroll(\n",
    "    note_info=performance,\n",
    "    # time_unit=\"auto\"\n",
    "    # time_div=\"auto\",\n",
    "    # onset_only=False,\n",
    "    # note_separation=False,\n",
    "    # remove_silence=True,\n",
    "    piano_range=use_piano_range,\n",
    "    # return_idxs=False,  \n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "420be1b4",
   "metadata": {},
   "source": [
    "Let's have a look at the output of these functions:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "athletic-woman",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pr"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "minute-enemy",
   "metadata": {},
   "source": [
    "By default, piano rolls computed with partitura are stored in scipy's sparse matrices, since most of the elements are 0.\n",
    "\n",
    "The first dimension of the array is MIDI pitch (128) and the second dimension are discrete time-steps defined by the `time_div` and `time_unit` arguments of the  `compute_pianoroll` function.\n",
    "\n",
    "Let's visualize the piano rolls!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "departmental-winner",
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize=(10, 7))\n",
    "axes[0].imshow(\n",
    "    score_pr.todense(), \n",
    "    aspect = \"auto\", \n",
    "    origin=\"lower\", \n",
    "    cmap=\"gray\",\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "axes[1].imshow(\n",
    "    performance_pr.todense(), \n",
    "    aspect = \"auto\", \n",
    "    origin=\"lower\", \n",
    "    cmap=\"gray\",\n",
    "    interpolation=\"nearest\",\n",
    ")\n",
    "y_label = \"Piano key\" if use_piano_range else \"MIDI pitch\"\n",
    "axes[0].set_ylabel(y_label)\n",
    "axes[1].set_ylabel(y_label)\n",
    "axes[1].set_xlabel(\"Time\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "atomic-addition",
   "metadata": {},
   "source": [
    "For more information, see the documentation of  [`compute_pianoroll`](https://partitura.readthedocs.io/en/latest/modules/partitura.utils.html#partitura.utils.compute_pianoroll)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "possible-oxford",
   "metadata": {},
   "source": [
    "### 2.2.2. Pitch Class Distributions\n",
    "\n",
    "These features are the symbolic equivalent to *chroma* features in audio. This representation is basically a piano roll that has been folded into a single octave."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "creative-fundamentals",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pc_pr = compute_pitch_class_pianoroll(\n",
    "    score,\n",
    "    normalize=True,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=4\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "urban-mediterranean",
   "metadata": {},
   "source": [
    "Let's plot this feature and compare it to a piano roll of the same score!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "turkish-sheriff",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pr = pt.utils.music.compute_pianoroll(\n",
    "    note_info=score,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=4,\n",
    "    piano_range=False\n",
    ")\n",
    "\n",
    "fig, axes = plt.subplots(2, figsize=(10, 5), sharex=True)\n",
    "\n",
    "axes[0].imshow(score_pc_pr, aspect = \"auto\", \n",
    "    origin=\"lower\", \n",
    "    cmap=\"gray\",\n",
    "    interpolation=\"nearest\",)\n",
    "axes[1].imshow(score_pr.todense(), \n",
    "               aspect=\"auto\", \n",
    "               origin=\"lower\", \n",
    "               cmap=\"gray\", \n",
    "               interpolation=\"nearest\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ab8d8d3c",
   "metadata": {},
   "source": [
    "## 2.3. Alignment Methods\n",
    "\n",
    "We move now to methods for computing the alignment between features from one version of a piece of music to another.\n",
    "\n",
    "Common methods are dynamic programming approaches like dynamic time warping (the focus of this tutorial) and probabilistic approaches like hidden Markov models. In this tutorial we are going to show how to perform alignment using DTW. \n",
    "\n",
    "Dynamic time warping (DTW) is a dynamic programming algorithm to find the **optimal** alignment between to time-dependent sequences. In a nutshell, the DTW algorithm finds the alignment between two sequence in three steps:\n",
    "\n",
    "1. Compute the pairwise distance between elements in sequence $\\mathbf{X}$ and $\\mathbf{Y}$.\n",
    "2. Compute the accumulated cost\n",
    "3. Find the best alignment by backtracking \n",
    "\n",
    "For now, let us generate some test data that we can play with!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "convinced-episode",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This is a helper method to generate sample sequences \n",
    "# (see helper.py for documentation)\n",
    "\n",
    "# lenght of the \"reference\" sequence\n",
    "lenX = 15\n",
    "\n",
    "# dimensionality of the feature space\n",
    "K = 5\n",
    "\n",
    "# This method generates an example sequence\n",
    "X, Y, gr_path = generate_example_sequences(\n",
    "    lenX=lenX, \n",
    "    centers=3, \n",
    "    n_features=K,\n",
    "    maxreps=4, \n",
    "    minreps=1, \n",
    "    noise_scale=0.1\n",
    ")\n",
    "\n",
    "# Let us plot the data to see how it looks like!\n",
    "plot_alignment(X, Y, gr_path)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "christian-briefing",
   "metadata": {},
   "source": [
    "### 2.3.1. Comparing the similarity of the features: Local cost distance\n",
    "\n",
    "We would like to know how to compare the elements in $\\mathbf{X}$ and $\\mathbf{Y}$. For this we use a local distance function, which can be any distance(-like) function that is small when $\\mathbf{x}_i$ is *similar* to $\\mathbf{y}_j$.\n",
    "\n",
    "Which distance to use depends on the problem at hand, although usual starting points are the Euclidean and the Manhattan ($L_1$) distances.\n",
    "\n",
    "Using this local distance, we can compare the elements in both sequences by comparing the pairwise distance of all elements in $\\mathbf{X}$ and $\\mathbf{Y}$. This will result in a matrix $\\mathbf{C}$, where the element $\\mathbf{C}[i,j]$ is given by\n",
    "\n",
    "$$\\mathbf{C}[i,j] = \\text{distance}(\\mathbf{x}_i, \\mathbf{y}_j)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "saved-slovenia",
   "metadata": {},
   "source": [
    "Let's visualize the pairwise cost matrix.\n",
    "\n",
    "See metrics implemented in [scipy.spatial.distance](https://docs.scipy.org/doc/scipy/reference/spatial.distance.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "mature-assessment",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alignment import pairwise_distance_matrix\n",
    "# Visualize cost matrix\n",
    "C = pairwise_distance_matrix(X, Y, metric='euclidean')\n",
    "plt.imshow(C, origin='lower', aspect='equal', cmap='gray')\n",
    "plt.xlabel(r'$\\mathbf{Y}$')\n",
    "plt.ylabel(r'$\\mathbf{X}$')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "forced-poster",
   "metadata": {},
   "source": [
    "The DTW Algorithm\n",
    "\n",
    "* **Input**: Cost matrix $\\mathbf{C}$ of size $N \\times M$\n",
    "* **Output**: Optimal warping path $P^*$, $d_{DTW}$\n",
    "\n",
    "**Procedure**\n",
    "\n",
    "1. Initialize $N \\times M$ matrix $D$ (accumulated cost) by\n",
    "\n",
    "$$\\mathbf{D}[n, 0] = \\sum_{k=0}^{n} \\mathbf{C}[k, 0]$$\n",
    "\n",
    "for $n \\in [0,N-1]$, and\n",
    "\n",
    "$$\\mathbf{D}[0, m] = \\sum_{k=0}^{m} \\mathbf{C}[0, k]$$\n",
    "\n",
    "for $n \\in [0, M-1]$\n",
    "\n",
    "2. Compute in a nested loop for $n=1,\\dots, N-1$ and $m=1, \\dots, M-1$\n",
    "\n",
    "$$\\mathbf{D}[n, m] = \\mathbf{C}[n, m] + \\min \\left\\{\\mathbf{D}[n-1, m-1], \\mathbf{D}[n-1, m], \\mathbf{D}[n, m-1] \\right\\}$$\n",
    "\n",
    "3. Get the warping path. Set $l = 0$ and $q_0 = (N-1, M-1)$. Repeat the following steps until $q_l = (0, 0)$\n",
    "    1. $l \\leftarrow l+1$ and $(n, m) = q_{l -1}$\n",
    "    2. If $n = 0$ then $q_l = (0, m - 1)$,\n",
    "    3. else if \n",
    "\n",
    "4. The dynamic time warping distance is given by\n",
    "\n",
    "$$d_{DTW}(\\mathbf{X}, \\mathbf{Y}) = D[N-1, M-1]$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "postal-might",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alignment import accumulated_cost_matrix\n",
    "\n",
    "D = accumulated_cost_matrix(C) \n",
    "\n",
    "# Visualize accumulated cost matrix\n",
    "plt.imshow(D, origin='lower', aspect='equal', cmap='gray')\n",
    "plt.xlabel(r'Sequence $\\mathbf{Y}$')\n",
    "plt.ylabel(r'Sequence $\\mathbf{X}$')\n",
    "plt.colorbar()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "nonprofit-improvement",
   "metadata": {},
   "outputs": [],
   "source": [
    "from alignment import optimal_warping_path\n",
    "        \n",
    "P = optimal_warping_path(D)\n",
    "\n",
    "plt.figure(figsize=(9, 3))\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.imshow(C, cmap='gray_r', origin='lower', aspect='equal')\n",
    "plt.plot(P[:, 1], P[:, 0], marker='o', color='r')\n",
    "plt.clim([0, np.max(C)])\n",
    "plt.colorbar()\n",
    "plt.title('$C$ with optimal warping path')\n",
    "plt.xlabel('Sequence Y')\n",
    "plt.ylabel('Sequence X')\n",
    "\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.imshow(D, cmap='gray_r', origin='lower', aspect='equal')\n",
    "plt.plot(P[:, 1], P[:, 0], marker='o', color='r')\n",
    "plt.plot(gr_path[:, 1], gr_path[:, 0], marker='d', color='purple', linewidth=1.1)\n",
    "plt.clim([0, np.max(D)])\n",
    "plt.colorbar()\n",
    "plt.title('$D$ with optimal warping path')\n",
    "plt.xlabel('Sequence Y')\n",
    "plt.ylabel('Sequence X')\n",
    "\n",
    "plt.tight_layout()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "attached-webster",
   "metadata": {},
   "source": [
    "This naive implementation is very slow! You can only use it for aligning small sequences. For practical stuff, we are going to use the `fasdtw` package. This package contains an efficient implementation of vanilla DTW, as well as a faster approximation, called FastDTW."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bronze-taste",
   "metadata": {},
   "outputs": [],
   "source": [
    "# The alignment.py module includes\n",
    "from alignment import dynamic_time_warping, fast_dynamic_time_warping\n",
    "import time\n",
    "\n",
    "for lenX in [10, 100, 1000]:\n",
    "    X, Y, gr_path = generate_example_sequences(\n",
    "        lenX=lenX, \n",
    "        centers=3, \n",
    "        n_features=K,\n",
    "        maxreps=2, \n",
    "        minreps=2, \n",
    "        noise_scale=0.1\n",
    "    )\n",
    "    st = time.process_time()\n",
    "    path_naive, dtwd_naive = dynamic_time_warping(X, Y, return_distance=True)\n",
    "    et_naive = time.process_time() - st\n",
    "\n",
    "    st = time.process_time()\n",
    "    path_fdtw, dtwd_fdtw = fast_dynamic_time_warping(X, Y, return_distance=True)\n",
    "    et_fdtw = time.process_time() - st\n",
    "\n",
    "    print(f\"Input sizes: X:{X.shape} Y:{Y.shape}\")\n",
    "    print(f\"\\tDTW: {dtwd_naive:.3f} ({et_naive * 1000:.2f} ms)\") \n",
    "    print(f\"\\tFastDTW: {dtwd_fdtw:.3f} ({et_fdtw * 1000:.2f} ms)\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "collective-allergy",
   "metadata": {},
   "source": [
    "### 2.3.2. Creating note-level alignments from sequential alignment information\n",
    "\n",
    "Dynamic Time Warping and related sequence alignment algorithms return a path between two sequences or time series. Note alignment of two polyphonic parts is categorically different from a time series alignment. To get to a note alignment, we need to figure out what notes are played at a specific time in the piano roll. Sometimes this information might be imprecise so we need to relax the search for notes at some piano roll time to find all relevant notes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "double-lancaster",
   "metadata": {},
   "outputs": [],
   "source": [
    "score_pr, sidx = pt.utils.music.compute_pianoroll(\n",
    "    note_info=score,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=8,\n",
    "    return_idxs=True,\n",
    "    piano_range=True,\n",
    "    binary=True,\n",
    "    note_separation=True,\n",
    ")\n",
    "\n",
    "performance_pr, pidx = pt.utils.music.compute_pianoroll(\n",
    "    note_info=performance,\n",
    "    time_unit=\"sec\",\n",
    "    time_div=10,\n",
    "    return_idxs=True,\n",
    "    piano_range=True,\n",
    "    binary=True,\n",
    "    note_separation=True,\n",
    ")\n",
    "\n",
    "reference_features = score_pr.todense().T\n",
    "performance_features = performance_pr.todense().T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ordinary-multiple",
   "metadata": {},
   "outputs": [],
   "source": [
    "# pitch_index, onset, offset, midi_pitch\n",
    "sidx[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "painful-latin",
   "metadata": {},
   "outputs": [],
   "source": [
    "# idx correspond to notes in note_array\n",
    "snote_array = score.note_array()\n",
    "print(snote_array[:5])\n",
    "\n",
    "# Check that the pitch in the note array corresponds to\n",
    "# the fourth column in the indices from the note array\n",
    "assert(all(snote_array[\"pitch\"] == sidx[:, 3]))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "demonstrated-confusion",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Dynamic time warping\n",
    "dtw_alignment = fast_dynamic_time_warping(\n",
    "    X=reference_features, \n",
    "    Y=performance_features, \n",
    "    metric=\"cityblock\",\n",
    ")\n",
    "\n",
    "print(dtw_alignment)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "buried-vermont",
   "metadata": {},
   "outputs": [],
   "source": [
    "from helper import greedy_note_alignment\n",
    "note_alignment = greedy_note_alignment(\n",
    "    warping_path=dtw_alignment, \n",
    "    idx1=sidx, \n",
    "    note_array1=score.note_array(), \n",
    "    idx2=pidx, \n",
    "    note_array2=performance.note_array()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1adf0cd4",
   "metadata": {},
   "outputs": [],
   "source": [
    "note_alignment[:5]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c9b2ac9e",
   "metadata": {},
   "source": [
    "## 2.4. Comparing alignments\n",
    "\n",
    "Let's compare different alignment methods"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4397d912",
   "metadata": {},
   "outputs": [],
   "source": [
    "# This file contains the ground truth alignment\n",
    "# gt_alignment_fn = \"content/mozart_k265_var1.match\"\n",
    "\n",
    "# This file contains the ground truth alignment\n",
    "gt_alignment_fn = os.path.join(MATCH_DIR, \"Chopin_op10_no3_p01.match\")\n",
    "\n",
    "# Load the alignment and the performance\n",
    "performance, gt_alignment = pt.load_match(\n",
    "    gt_alignment_fn, \n",
    "    pedal_threshold=127, \n",
    "    first_note_at_zero=True\n",
    ")\n",
    "pnote_array = performance.note_array()\n",
    "\n",
    "# Load the score\n",
    "score_fn = os.path.join(MUSICXML_DIR, \"Chopin_op10_no3.musicxml\")\n",
    "score = pt.load_score(score_fn)\n",
    "snote_array = score.note_array()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c136d749",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the features\n",
    "score_pcr, sidx = pt.utils.music.compute_pitch_class_pianoroll(\n",
    "    note_info=score,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=8,\n",
    "    return_idxs=True,\n",
    "    binary=True,\n",
    "    note_separation=True,\n",
    ")\n",
    "\n",
    "performance_pcr, pidx = pt.utils.music.compute_pitch_class_pianoroll(\n",
    "    note_info=performance,\n",
    "    time_unit=\"sec\",\n",
    "    time_div=8,\n",
    "    return_idxs=True,\n",
    "    binary=True,\n",
    "    note_separation=True,\n",
    ")\n",
    "\n",
    "reference_features = score_pcr.T\n",
    "performance_features = performance_pcr.T"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c113c6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# DTW\n",
    "dtw_pcr_warping_path = fast_dynamic_time_warping(\n",
    "    X=reference_features, \n",
    "    Y=performance_features, \n",
    "    metric=\"cityblock\",\n",
    ")\n",
    "\n",
    "dtw_pcr_alignment = greedy_note_alignment(\n",
    "    warping_path=dtw_pcr_warping_path, \n",
    "    idx1=sidx, \n",
    "    note_array1=snote_array, \n",
    "    idx2=pidx, \n",
    "    note_array2=pnote_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b491caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Compute the features\n",
    "score_pr, sidx = pt.utils.music.compute_pianoroll(\n",
    "    note_info=score,\n",
    "    time_unit=\"beat\",\n",
    "    time_div=8,\n",
    "    return_idxs=True,\n",
    "    piano_range=True,\n",
    "    binary=True,\n",
    "    note_separation=True,\n",
    ")\n",
    "\n",
    "performance_pr, pidx = pt.utils.music.compute_pianoroll(\n",
    "    note_info=performance,\n",
    "    time_unit=\"sec\",\n",
    "    time_div=8,\n",
    "    return_idxs=True,\n",
    "    piano_range=True,\n",
    "    binary=True,\n",
    "    note_separation=True,\n",
    ")\n",
    "\n",
    "reference_features = score_pr.toarray().T\n",
    "performance_features = performance_pr.toarray().T\n",
    "\n",
    "# DTW\n",
    "dtw_pr_warping_path = fast_dynamic_time_warping(\n",
    "    X=reference_features, \n",
    "    Y=performance_features, \n",
    "    metric=\"cityblock\",\n",
    ")\n",
    "\n",
    "dtw_pr_alignment = greedy_note_alignment(\n",
    "    warping_path=dtw_pr_warping_path, \n",
    "    idx1=sidx, \n",
    "    note_array1=snote_array, \n",
    "    idx2=pidx, \n",
    "    note_array2=pnote_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5103876b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# invent a linear alignment for testing\n",
    "from helper import dummy_linear_alignment\n",
    "\n",
    "# Dummy linear alignment\n",
    "linear_warping_path = dummy_linear_alignment(\n",
    "    X=reference_features, \n",
    "    Y=performance_features,\n",
    ")\n",
    "\n",
    "linear_alignment = greedy_note_alignment(\n",
    "    warping_path=linear_warping_path, \n",
    "    idx1=sidx, \n",
    "    note_array1=snote_array, \n",
    "    idx2=pidx, \n",
    "    note_array2=pnote_array,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "tired-sunday",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(linear_warping_path[:, 0], linear_warping_path[:, 1], label=\"linear\")\n",
    "plt.plot(dtw_pr_warping_path[:, 0], dtw_pr_warping_path[:, 1], label=\"DTW (piano roll)\")\n",
    "plt.plot(dtw_pcr_warping_path[:, 0], dtw_pcr_warping_path[:, 1], label=\"DTW (pitch class)\")\n",
    "plt.legend()\n",
    "plt.xlabel('Index in score')\n",
    "plt.ylabel('Index in performance')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wireless-projection",
   "metadata": {},
   "source": [
    "To inspect an alignment, we can use [**Parangonada**](https://sildater.github.io/parangonada/), a tool to compare alignments developed at our institute!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "specific-receiver",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Export files to Parangonada\n",
    "outdir = \"parangonada_files\"\n",
    "if not os.path.exists(outdir):\n",
    "    os.mkdir(outdir)\n",
    "pt.save_parangonada_csv(\n",
    "    alignment=dtw_pr_alignment,\n",
    "    performance_data=performance,\n",
    "    score_data=score,\n",
    "    zalign=linear_alignment,\n",
    "    outdir=\"parangonada_files\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "coral-integral",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "from helper import evaluate_alignment_notewise\n",
    "\n",
    "print(f\"Method\\tF-score\\tPrecision\\tRecall\")\n",
    "method = \"linear\"\n",
    "\n",
    "methods = [\n",
    "    (linear_alignment, \"linear\"),\n",
    "    (dtw_pr_alignment, \"DTW (piano roll)\"),\n",
    "    (dtw_pcr_alignment, \"DTW (pitch class)\"),\n",
    "]\n",
    "\n",
    "for align, method in methods:\n",
    "    precision, recall, fscore = evaluate_alignment_notewise(\n",
    "        prediction=align,\n",
    "        ground_truth=gt_alignment\n",
    "    )\n",
    "    print(f\"{method}\\t{fscore:.4f}\\t{precision:.4f}\\t{recall:.4f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "alike-doctor",
   "metadata": {},
   "source": [
    "## 2.5. Alignment Applications\n",
    "\n",
    "In this example, we are going to compare tempo curves of different performances of the same piece. Partitura includes a utility function called `get_time_maps_from_alignment`which creates functions (instances of [`scipy.interpolate.interp1d`](https://docs.scipy.org/doc/scipy/reference/generated/scipy.interpolate.interp1d.html)) that map score time to performance time (and the other way around)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "offshore-bridal",
   "metadata": {},
   "outputs": [],
   "source": [
    "# get all match files\n",
    "piece = \"Chopin_op10_no3\"\n",
    "matchfiles = glob.glob(os.path.join(MATCH_DIR, f\"{piece}_p*.match\"))\n",
    "matchfiles.sort()\n",
    "\n",
    "# Load the score\n",
    "score_fn = os.path.join(MUSICXML_DIR, f\"{piece}.musicxml\")\n",
    "score = pt.load_score(score_fn)\n",
    "score_part = score[0]\n",
    "snote_array = score.note_array()\n",
    "\n",
    "# Score time from the first to the last onset\n",
    "score_time = np.linspace(snote_array['onset_beat'].min(),\n",
    "                         snote_array['onset_beat'].max(),\n",
    "                         100)\n",
    "# Include the last offset\n",
    "score_time_ending = np.r_[\n",
    "    score_time, \n",
    "    (snote_array['onset_beat'] + snote_array['duration_beat']).max() # last offset\n",
    "]\n",
    "\n",
    "tempo_curves = np.zeros((len(matchfiles), len(score_time)))\n",
    "for i, matchfile in enumerate(matchfiles):\n",
    "    # load alignment\n",
    "    perf, alignment = pt.load_match(matchfile)\n",
    "    # Get score time to performance time map\n",
    "    _, stime_to_ptime_map = pt.utils.music.get_time_maps_from_alignment(\n",
    "        perf, score, alignment)\n",
    "    # Compute naïve tempo curve\n",
    "    performance_time = stime_to_ptime_map(score_time_ending)\n",
    "    tempo_curves[i,:] = 60 * np.diff(score_time_ending) / np.diff(performance_time)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "brazilian-honey",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots(1, figsize=(15, 8))\n",
    "color = plt.cm.rainbow(np.linspace(0, 1, len(tempo_curves)))\n",
    "for i, tempo_curve in enumerate(tempo_curves):\n",
    "    ax.plot(score_time, tempo_curve, \n",
    "            label=f'pianist {i + 1:02d}', alpha=0.4, c=color[i])\n",
    "\n",
    "# plot average performance\n",
    "ax.plot(score_time, tempo_curves.mean(0), label='average', c='black', linewidth=2)\n",
    "\n",
    "# get starting time of each measure in the score\n",
    "measure_times = score_part.beat_map([measure.start.t for measure in score_part.iter_all(pt.score.Measure)])\n",
    "# do not include pickup measure\n",
    "measure_times = measure_times[measure_times >= 0]\n",
    "ax.set_title(piece)\n",
    "ax.set_xlabel('Score time (beats)')\n",
    "ax.set_ylabel('Tempo (bpm)')\n",
    "ax.set_xticks(measure_times)\n",
    "plt.legend(frameon=False, bbox_to_anchor = (1.15, .9))\n",
    "plt.grid(axis='x')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8650c3a0",
   "metadata": {},
   "source": [
    "## 2.6. References\n",
    "\n",
    "We include a few references to get you started in the topic of music alignment! Note that some of these references do not necessarily focus on symbolic music alignment, but on other modalities of music alignment.\n",
    "\n",
    "#### 2.6.1. Alignment Basics\n",
    "\n",
    "* M. Müller.\n",
    "\n",
    "* A. Arzt.\n",
    "\n",
    "#### 2.6.2. DTW-based alignment\n",
    "\n",
    "\n",
    "\n",
    "#### 2.6.3. HMM-based alignment\n",
    "\n",
    "\n",
    "\n",
    "#### 2.6.4. Other applications\n",
    "\n",
    "* N. Lizé-Masclef, A. Vaglio, M. Moussallam. “User-centered evaluation of lyrics to audio alignment”, International Society for Music Information Retrieval (ISMIR) conference, 2021.\n",
    "\n",
    "* M. Mauch, F: Hiromasa, M. Goto. “Lyrics-to-audio alignment and phrase-level segmentation using incomplete internet-style chord annotations”, Frontiers in Proceedings of the Sound Music Computing Conference (SMC), 2010.\n",
    "\n",
    "* G. Dzhambazov. “Knowledge-Based Probabilistic Modeling For Tracking Lyrics In Music Audio Signals”, PhD Thesis, 2017.\n",
    "\n",
    "* H. Fujihara, M. Goto, J. Ogata, H. Okuno. “LyricSynchronizer: Automatic synchronization system between musical audio signals and lyrics”, IEEE Journal of Selected Topics in Signal Processing, VOL. 5, NO. 6, 2011"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.10"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
