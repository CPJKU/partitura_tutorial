<!DOCTYPE html>
<head>
  <meta charset="utf-8">
  <meta name="viewport" content="width=device-width, initial-scale=1" />

  <meta name="viewport" content="width=device-width, initial-scale=1.0">
  <meta name="theme-color" content="#2D2D2D" />
  
  <title>Partitura Tutorial :: Drum Generation Transformer</title>
  

  <link rel="icon" type="image/png" sizes="32x32" href="../../_static/img/favicon-32x32.png">
  <link rel="icon" type="image/png" sizes="16x16" href="../../_static/img/favicon-16x16.png">
  <link rel="index" title="Index" href="../../genindex.html"/>

  <link rel="stylesheet" href="../../_static/css/insegel.css"/>
  <link rel="stylesheet" href="../../_static/css/custom.css"/>

  <script type="text/javascript" id="documentation_options" data-url_root="../../" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/documentation_options.js"></script>
      <script type="text/javascript" src="../../_static/doctools.js"></script>
      <script type="text/javascript" src="../../_static/sphinx_highlight.js"></script>
      <script type="text/javascript" src="../../https://cdnjs.cloudflare.com/ajax/libs/require.js/2.3.4/require.min.js"></script>
      <script type="text/javascript" src="../../"></script>
      <script type="text/javascript" src="../../https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
  

  <script src="https://email.tl.fortawesome.com/c/eJxNjUEOgyAQAF8jR7Kw6wIHDh7sP1Cw2mgxgmn6-3JsMqc5zEQfE8dkxOY1KKMUOI3ACFKRJpSW2AAp7ontYIaxI6i7XPJVwyeVfCQ550Os3jLrGSNOLgbdAy6s0PBk2TFNjEbsfq31LB0OnX407pJa5v2faRadwSW63mn5KuLyR9j2tgx3zecanl-55R_-jjPs"></script> 
</head>

<body>
  <div id="insegel-container">
    <header>
      <div id="logo-container">
          
          <h1>Partitura Tutorial</h1>
          
      </div>
      <div id="project-container">
        
        <h1>Documentation</h1>
        
      </div>
    </header>

    <div id="content-container">

      <div id="main-content-container">
        <div id="main-content" role="main">
          
  <p><a class="reference external" href="https://colab.research.google.com/github/CPJKU/partitura_tutorial/blob/main/notebooks/04_generation/Drum_Generation_Transformer.ipynb"><img alt="Open in Colab" src="%22https://colab.research.google.com/assets/colab-badge.svg%22" /></a></p>
<section id="Drum-Generation-Transformer">
<h1>Drum Generation Transformer<a class="headerlink" href="#Drum-Generation-Transformer" title="Link to this heading">¶</a></h1>
<p>Welcome to this partitura tutorial notebook! In this tutorial, we will learn how to train a small auto-regressive transformer model for simplified drum beat generation in MIDI. At the end of this tutorial, we will be able to create a drum beat of a bar’s length and investigate some useful techniques for encoding MIDI as input to a transformer.</p>
<section id="Setup">
<h2>Setup<a class="headerlink" href="#Setup" title="Link to this heading">¶</a></h2>
<p><strong>Install packages</strong></p>
<p>If you run it locally, we assume that you cloned the full tutorial repository from <a class="reference external" href="https://github.com/CPJKU/partitura_tutorial.git">https://github.com/CPJKU/partitura_tutorial.git</a>, you are running this notebook from its parent path, and have partitura and the other dependencies installed. Partitura is available in github <a class="reference external" href="https://github.com/CPJKU/partitura">https://github.com/CPJKU/partitura</a> You can install it with:</p>
<div class="highlight-none notranslate"><div class="highlight"><pre><span></span>pip install partitura
</pre></div>
</div>
<p>If any other imports fail, please install them in your local environment. If you run it in colab, partitura and other dependencies will be installed automatically.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[1]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>try:
    import google.colab
    IN_COLAB = True
except:
    IN_COLAB = False

if IN_COLAB:
    # Issues on Colab with newer versions of MIDO
    # this install should be removed after the following
    # pull request is accepted in MIDO
    # https://github.com/mido/mido/pull/584
    ! pip install mido==1.2.10
    !pip install partitura
    !git clone https://github.com/cpjku/partitura_tutorial
    import sys
    sys.path.insert(0, &quot;./partitura_tutorial/notebooks/04_generation/&quot;)
</pre></div>
</div>
</div>
<section id="Imports">
<h3>Imports<a class="headerlink" href="#Imports" title="Link to this heading">¶</a></h3>
<p>Import the required packages.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[2]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>%matplotlib inline
import numpy as np
import pandas as pd
import random
import time
import glob
import matplotlib.pyplot as plt
import copy
import pickle
import warnings
warnings.simplefilter(&quot;ignore&quot;, UserWarning)

import torch
from torch import nn, optim
from torch.utils.data import Dataset, ConcatDataset, DataLoader
from torch.nn import TransformerEncoder, TransformerEncoderLayer
import os
if not IN_COLAB:
    os.environ[&#39;KMP_DUPLICATE_LIB_OK&#39;]=&#39;True&#39;
import partitura as pt
</pre></div>
</div>
</div>
</section>
<section id="External-files">
<h3>External files<a class="headerlink" href="#External-files" title="Link to this heading">¶</a></h3>
<p>If run locally, external scripts are loaded directly from the cloned tutorial repository.</p>
<p>If run in colab, external scripts are imported by httpimport.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[3]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from generation_helpers import (
    INV_PITCH_DICT_SIMPLE,
    tokens_2_notearray,
    save_notearray_2_midifile,
    generate_tokenized_data,
    batch_data,
    PositionalEncoding,
    Transformer,
    sample_from_logits,
    sample_loop
    )
</pre></div>
</div>
</div>
</section>
</section>
<section id="Data-Loading">
<h2>Data Loading<a class="headerlink" href="#Data-Loading" title="Link to this heading">¶</a></h2>
<p>I this tutorial we will work with the groove midi dataset, which contains MIDI drum grooves from a multitude of genres. For simplicity we load only the MIDI files in 4-4 from the dataset. For loading we use <em>Partitura</em>. If you want to load a precomputed and preprocessed dataset you can skip the next cells until the last cell before section 4 where it is loaded.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[4]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>if IN_COLAB:
    directory = os.path.join(&quot;./partitura_tutorial/notebooks/04_generation&quot;, &quot;./groove-v1.0.0-midionly&quot;)
else:
    directory = os.path.join(os.getcwd(), &quot;./groove-v1.0.0-midionly&quot;)

typ_44 = [os.path.basename(f) for f in list(
    glob.glob(directory + &quot;/groove/*/*/*.mid&quot;))
          if &quot;4-4&quot; in f]

beat_type = [g.split(&quot;_&quot;)[-2] for g  in typ_44]
tempo = [int(g.split(&quot;_&quot;)[-3]) for g  in typ_44]
vals, counts = np.unique(tempo, return_counts = True)
</pre></div>
</div>
</div>
<section id="Histogram-of-tempos-marked-in-file-names">
<h3>Histogram of tempos marked in file names<a class="headerlink" href="#Histogram-of-tempos-marked-in-file-names" title="Link to this heading">¶</a></h3>
<p>Let’s take a closer look at the data by plotting the histogram of tempi from the target pieces.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.plot(vals, counts, c = &quot;r&quot;)
plt.hist(np.array(tempo), bins= 60)
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[5]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
(array([  1.,   0.,  10.,  13.,  15.,  18.,  19.,  36.,  42.,  24., 104.,
        110.,  85.,  69.,   8.,  69.,  91., 102.,  82.,  39.,  51.,  16.,
         60.,  42.,  10.,   1.,   0.,   2.,   0.,   0.,   2.,   1.,   3.,
          3.,   0.,   1.,   0.,   1.,   0.,   0.,   0.,   7.,   0.,   0.,
          0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,   0.,
          0.,   0.,   0.,   0.,   1.]),
 array([ 50.,  54.,  58.,  62.,  66.,  70.,  74.,  78.,  82.,  86.,  90.,
         94.,  98., 102., 106., 110., 114., 118., 122., 126., 130., 134.,
        138., 142., 146., 150., 154., 158., 162., 166., 170., 174., 178.,
        182., 186., 190., 194., 198., 202., 206., 210., 214., 218., 222.,
        226., 230., 234., 238., 242., 246., 250., 254., 258., 262., 266.,
        270., 274., 278., 282., 286., 290.]),
 &lt;BarContainer object of 60 artists&gt;)
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_04_generation_Drum_Generation_Transformer_10_1.png" src="../../_images/notebooks_04_generation_Drum_Generation_Transformer_10_1.png" />
</div>
</div>
<p>We can almost notice a Gaussian curve with a mean around a 100 beats per minute. Now let’s define the <code class="docutils literal notranslate"><span class="pre">load_data</span></code> fun</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[6]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def load_data(
      directory = &quot;./groove-v1.0.0-midionly&quot;,
      min_seq_length=10,
      time_sig=&quot;4-4&quot;,
      beat_type = None
              ):
    &quot;&quot;&quot;
    loads groove dataset data from directory
    into a list of dictionaries containing the
    note array as well as note sequence metadata.

    Args:
        directory: dir of the dataset
        min_seq_length: minimal sequence length, a
            all shorter performances are discarded
        time_sig: only performances of this time sig
            are loaded
        beat_type: type of beat to load (&quot;beat&quot;,
            &quot;fill&quot;, or None = both)

    Returns:
        a list of dicts containing note arrays.

    &quot;&quot;&quot;
    # load data
    files = glob.glob(directory + &quot;/groove/*/*/*.mid&quot;)
    files.sort()
    sequences = []
    if beat_type is None:
        beat_types = [&quot;beat&quot;, &quot;fill&quot;]
    else:
        beat_types = [beat_type]
    for fn in files:
        bn = os.path.basename(fn)
        bns = bn.split(&quot;_&quot;)
        tempo = int(bns[-3])
        bt = bns[-2]
        ts = bns[-1].split(&#39;.&#39;)[-2]
        if ts == time_sig and bt in beat_types:
            seq = pt.load_performance_midi(fn)[0]
            if len(seq.notes) &gt; min_seq_length:
                na = seq.note_array()
                namax = (na[&#39;onset_tick&#39;] + na[&#39;duration_tick&#39;]).max()
                namin = (na[&#39;onset_tick&#39;]).min()
                dur_in_q = (namax - namin)/seq.ppq
                seq_object = {
                    &quot;id&quot;: bn,
                    &quot;na&quot;: na,
                    &quot;ppq&quot;: seq.ppq,
                    &quot;tempo&quot;: tempo,
                    &quot;beat_type&quot;: bt,
                    &quot;namax&quot;: namax,
                    &quot;namin&quot;: namin,
                    &quot;dur_in_q&quot;: dur_in_q
                }
                sequences.append(seq_object)
    return sequences
</pre></div>
</div>
</div>
<section id="Call-the-loading-function-(might-take-a-moment)">
<h4>Call the loading function (might take a moment)<a class="headerlink" href="#Call-the-loading-function-(might-take-a-moment)" title="Link to this heading">¶</a></h4>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[7]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>seqs = load_data(directory = directory)
</pre></div>
</div>
</div>
</section>
</section>
<section id="Histogram-of-performance-durations">
<h3>Histogram of performance durations<a class="headerlink" href="#Histogram-of-performance-durations" title="Link to this heading">¶</a></h3>
<p>In this visualization example for the loaded data we plot the histogram of note durations.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[8]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>dur_in_quarters = [k[&quot;dur_in_q&quot;] for k in seqs if k[&quot;beat_type&quot;] == &quot;beat&quot;]
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[9]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>plt.hist(dur_in_quarters, bins = 60)
liq = np.array(dur_in_quarters)
print((liq &gt;= 4).sum(), (liq &gt;= -4).sum())
</pre></div>
</div>
</div>
<div class="nboutput docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
470 484
</pre></div></div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<img alt="../../_images/notebooks_04_generation_Drum_Generation_Transformer_17_1.png" src="../../_images/notebooks_04_generation_Drum_Generation_Transformer_17_1.png" />
</div>
</div>
</section>
</section>
<section id="3.-Data-Preprocessing:-Segmentation-&amp;-Tokenization">
<h2>3. Data Preprocessing: Segmentation &amp; Tokenization<a class="headerlink" href="#3.-Data-Preprocessing:-Segmentation-&-Tokenization" title="Link to this heading">¶</a></h2>
<p><strong>Reducing and Encoding the MIDI Sound Profiles</strong></p>
<p>In this reduces drum generation we encode only 6 main sounds from a list of 22 standard perscussion MIDI sound slots.</p>
<p>Here is a table that shows the Typical Sound to pitch association for drums in MIDI:</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p><strong>MIDI PITCH</strong></p></th>
<th class="head"><p><strong>STANDARD ASSOCIATED SOUND</strong></p></th>
<th class="head"><p><strong>REWRITEN SOUND</strong></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>36</p></td>
<td><p>Kick</p></td>
<td><p>Kick</p></td>
</tr>
<tr class="row-odd"><td><p>37</p></td>
<td><p>Snare (X-stick)</p></td>
<td><p>Snare</p></td>
</tr>
<tr class="row-even"><td><p>38</p></td>
<td><p>Snare (Head)</p></td>
<td><p>Snare</p></td>
</tr>
<tr class="row-odd"><td><p>40</p></td>
<td><p>Snare (Rim)</p></td>
<td><p>Snare</p></td>
</tr>
<tr class="row-even"><td><p>43</p></td>
<td><p>Tom 1</p></td>
<td><p>Tom (Generic)</p></td>
</tr>
<tr class="row-odd"><td><p>45</p></td>
<td><p>Tom 2</p></td>
<td><p>Tom (Generic)</p></td>
</tr>
<tr class="row-even"><td><p>48</p></td>
<td><p>Tom 3</p></td>
<td><p>Tom (Generic)</p></td>
</tr>
<tr class="row-odd"><td><p>47</p></td>
<td><p>Tom 2 (Rim)</p></td>
<td><p>Tom (Generic)</p></td>
</tr>
<tr class="row-even"><td><p>50</p></td>
<td><p>Tom 1 (Rim)</p></td>
<td><p>Tom (Generic)</p></td>
</tr>
<tr class="row-odd"><td><p>48</p></td>
<td><p>Tom 3 (Rim)</p></td>
<td><p>Tom (Generic)</p></td>
</tr>
<tr class="row-even"><td><p>48</p></td>
<td><p>Hi-Hat Closed (Bow)</p></td>
<td><p>Hi-Hat</p></td>
</tr>
<tr class="row-odd"><td><p>48</p></td>
<td><p>Hi-Hat Closed (Edge)</p></td>
<td><p>Hi-Hat</p></td>
</tr>
<tr class="row-even"><td><p>48</p></td>
<td><p>Hi-Hat Open (Bow)</p></td>
<td><p>Hi-Hat</p></td>
</tr>
<tr class="row-odd"><td><p>48</p></td>
<td><p>Hi-Hat Open (Edge)</p></td>
<td><p>Hi-Hat</p></td>
</tr>
<tr class="row-even"><td><p>49</p></td>
<td><p>Crash 1</p></td>
<td><p>Crash</p></td>
</tr>
<tr class="row-odd"><td><p>55</p></td>
<td><p>Crash 1</p></td>
<td><p>Crash</p></td>
</tr>
<tr class="row-even"><td><p>52</p></td>
<td><p>Crash 2</p></td>
<td><p>Crash</p></td>
</tr>
<tr class="row-odd"><td><p>57</p></td>
<td><p>Crash 2</p></td>
<td><p>Crash</p></td>
</tr>
<tr class="row-even"><td><p>51</p></td>
<td><p>Ride (bow)</p></td>
<td><p>Ride</p></td>
</tr>
<tr class="row-odd"><td><p>59</p></td>
<td><p>Ride (Edge)</p></td>
<td><p>Ride</p></td>
</tr>
<tr class="row-even"><td><p>53</p></td>
<td><p>Ride (Bell)</p></td>
<td><p>Ride</p></td>
</tr>
<tr class="row-odd"><td><p>Any</p></td>
<td><p><em>Default</em></p></td>
<td><p><em>Default</em></p></td>
</tr>
</tbody>
</table>
<p>In summary, we associate all the standard MIDI drum sounds to their sound <em>family</em>, i.e. Snare-type sounds to a Signle Snare sound, and so on.</p>
<p><strong>Reducing the MIDI Velocity Encoding</strong></p>
<p>For MIDI velocity values we encode 8 classes of velocity by applying:</p>
<div class="math notranslate nohighlight">
\[newVel = oldVel / 2^4\]</div>
<p>We only accept integer values for <span class="math notranslate nohighlight">\(newVel\)</span>, so we are left with 8 classes of velocity.</p>
<p><strong>Reducing the Tempo Encoding</strong></p>
<p>Similarly, we reduce the different tempi found in the training data into 8 classses.</p>
<p><strong>Beat of Fill Characterization</strong></p>
<p>We also add information about each measure of the data being a beat or fill/solo part.</p>
<p><strong>Reducing the Time Encoding</strong></p>
<p>For the time encoding we use a hierarchical tree division in base 2. We start by separating the bar in 2, for first and second half note of the 4/4 bar. We continue for quarter, eight notes, and so on, up to 128 notes. So we use a binary setting for every hierarchical level.</p>
<p>For example the binary hierarchical representation of the second 16th note of the bar would be :</p>
<table class="docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>Half Note</p></th>
<th class="head"><p>Quarter Note</p></th>
<th class="head"><p>8-th Note</p></th>
<th class="head"><p>16-th Note</p></th>
<th class="head"><p>32-th Note</p></th>
<th class="head"><p>64-th Note</p></th>
<th class="head"><p>128-th Note</p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>1</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
<td><p>0</p></td>
</tr>
</tbody>
</table>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[10]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>PITCH_DICT_SIMPLE = {
    36:0, #Kick
    38:1, #Snare (Head)
    40:1, #Snare (Rim)
    37:1, #Snare X-Stick
    48:2, #Tom 1
    50:2, #Tom 1 (Rim)
    45:2, #Tom 2
    47:2, #Tom 2 (Rim)
    43:2, #Tom 3
    58:2, #Tom 3 (Rim)
    46:3, #HH Open (Bow)
    26:3, #HH Open (Edge)
    42:3, #HH Closed (Bow)
    22:3, #HH Closed (Edge)
    44:3, #HH Pedal
    49:4, #Crash 1
    55:4, #Crash 1
    57:4, #Crash 2
    52:4, #Crash 2
    51:5, #Ride (Bow)
    59:5, #Ride (Edge)
    53:5, #Ride (Bell)
    &#39;default&#39;:6
}

def pitch_encoder(pitch, pitch_dict = PITCH_DICT_SIMPLE):
    # 22 different instruments  in dataset, default 23rd class
    a = pitch_dict[&#39;default&#39;]
    try:
        a = pitch_dict[pitch]
    except:
        print(&quot;unknown instrument&quot;)
    return a

def time_encoder(time_div, ppq):
    # base 2 encoding of time, starting at half note, ending at 128th
    power_two_classes = list()
    for i in range(7):
        power_two_classes.append(int(time_div // (ppq * (2 **(1-i)))))
        time_div = time_div % (ppq * 2 **(1-i))
    return power_two_classes

def velocity_encoder(vel):
    # 8 classes of velocity
    return vel // 2 ** 4

def tempo_encoder(tmp):
    # 8 classes of tempo between 60 and 180
    return (np.clip(tmp, 60, 179) - 60) // 15

def tokenizer(seq):
    tokens = list()
    na = seq[&quot;na&quot;]
    fill_encoding = 0
    if seq[&quot;beat_type&quot;] == &quot;fill&quot;:
        fill_encoding = 1
    tempo_encoding = tempo_encoder(seq[&quot;tempo&quot;])

    for note in na:
        te = time_encoder(note[&quot;onset_tick&quot;]%(seq[&#39;ppq&#39;]*4), seq[&#39;ppq&#39;])
        pe = pitch_encoder(note[&quot;pitch&quot;])
        ve = velocity_encoder(note[&quot;velocity&quot;])

        tokens.append(te + [pe, ve, tempo_encoding, fill_encoding])

    return tokens

def measure_segmentation(seq, beats = 4, minimal_notes = 1):
    mod = beats * seq[&#39;ppq&#39;]
    no_of_measures = int(seq[&quot;namax&quot;] // mod + 1)

    segmented_seq = list()
    for measure_idx in range(no_of_measures):
        na = seq[&quot;na&quot;]
        new_na = np.copy(na[na[&quot;onset_tick&quot;] // mod == measure_idx])
        if len(new_na) &gt;= minimal_notes:
            new_seq = copy.copy(seq)
            new_seq[&quot;na&quot;] = new_na
            segmented_seq.append(new_seq)
        else:
            continue
    return segmented_seq
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[11]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># try uncommenting the next lines to see the outputs of these functions.
t = tokenizer(seqs[0])
ss = measure_segmentation(seqs[0], minimal_notes = 18)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[12]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>print(&quot;First Tokenized Note of First sequence : {}&quot;.format(t[0]))
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
First Tokenized Note of First sequence : [0, 0, 1, 1, 1, 0, 1, 0, 3, 3, 0]
</pre></div></div>
</div>
<section id="Training-set-generation-(might-take-a-moment)">
<h3>Training set generation (might take a moment)<a class="headerlink" href="#Training-set-generation-(might-take-a-moment)" title="Link to this heading">¶</a></h3>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[13]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train_data = generate_tokenized_data(seqs,
                                     measure_segmentation,
                                     tokenizer,
                                     minimal_notes = 20)
train_dataloader = batch_data(train_data,
                              batch_size=128)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
76 batches of size 128
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[14]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>## try uncommenting the next lines to see the outputs of these functions.
# train_dataloader[0].shape # batch, max sequence length, num_tokens
</pre></div>
</div>
</div>
</section>
<section id="Saving-and-loading-a-preprocessed-training-set">
<h3>Saving and loading a preprocessed training set<a class="headerlink" href="#Saving-and-loading-a-preprocessed-training-set" title="Link to this heading">¶</a></h3>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[15]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># with open(&#39;./dataset129.pyc&#39;, &#39;wb&#39;) as fh:
#     pickle.dump(train_dataloader, fh)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[16]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span># with open(&#39;./dataset129.pyc&#39;, &#39;rb&#39;) as fh:
#     train_dataloader = pickle.load(fh)
</pre></div>
</div>
</div>
</section>
</section>
<section id="4.-Model">
<h2>4. Model<a class="headerlink" href="#4.-Model" title="Link to this heading">¶</a></h2>
<p><img alt="MODEL" src="https://raw.githubusercontent.com/CPJKU/partitura_tutorial/mlflow/static/drumtransformer.gif" /></p>
<p>The embedding input and output dimension of our model uses the reduced representations that were described above. First, we input with the 7 levels binary hierarchical timing representation in one hot, next instrument (6 classes), velocity (6 classes), tempo (6 classes), and a binary beat as one hot. For the output, every one of those levels also contain a <em>START</em> or <em>END</em> token. Therefore, the binary one-hot representations go from 2 dimensional vectors to 4 dimensional where the third position
is for the <em>START</em> token and the fourth position is for the <em>END</em> token.</p>
<p>For consistency we repeated the same dimensions for the input Embeddings of the model. Of course this encodings are up to personal interpretation</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[17]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>class MultiEmbedding(nn.Module):
    &#39;&#39;&#39;A Multiembedding&#39;&#39;&#39;
    # Initialization
    def __init__(self, tokens2dims):
        super().__init__()
        # self.em_time0 = nn.Embedding(num_tokens, dim_model)
        self.em_time0 = nn.Embedding(tokens2dims[0][0], tokens2dims[0][1])
        self.em_time1 = nn.Embedding(tokens2dims[1][0], tokens2dims[1][1])
        self.em_time2 = nn.Embedding(tokens2dims[2][0], tokens2dims[2][1])
        self.em_time3 = nn.Embedding(tokens2dims[3][0], tokens2dims[3][1])
        self.em_time4 = nn.Embedding(tokens2dims[4][0], tokens2dims[4][1])
        self.em_time5 = nn.Embedding(tokens2dims[5][0], tokens2dims[5][1])
        self.em_time6 = nn.Embedding(tokens2dims[6][0], tokens2dims[6][1])
        self.em_pitch = nn.Embedding(tokens2dims[7][0], tokens2dims[7][1])
        self.em_velocity = nn.Embedding(tokens2dims[8][0], tokens2dims[8][1])
        self.em_tempo = nn.Embedding(tokens2dims[9][0], tokens2dims[9][1])
        self.em_beat = nn.Embedding(tokens2dims[10][0], tokens2dims[10][1])
        # self.total_dim = np.array([[2,2,2,2,2,2,2, # time encoding
        #                  2,2,2,2]]).sum() # instrument, velocity, tempo, beat/fill
    def forward(self, x):
        &#39;&#39;&#39;Update Function and predict function of the model&#39;&#39;&#39;

        output = torch.cat((
        self.em_time0(x[:,:,0]),
        self.em_time1(x[:,:,1]),
        self.em_time2(x[:,:,2]),
        self.em_time3(x[:,:,3]),
        self.em_time4(x[:,:,4]),
        self.em_time5(x[:,:,5]),
        self.em_time6(x[:,:,6]),
        self.em_pitch(x[:,:,7]),
        self.em_velocity(x[:,:,8]),
        self.em_tempo(x[:,:,9]),
        self.em_beat(x[:,:,10])),
        dim=-1)
        return output
</pre></div>
</div>
</div>
</section>
<section id="5.-Training">
<h2>5. Training<a class="headerlink" href="#5.-Training" title="Link to this heading">¶</a></h2>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[18]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def train_loop(model, opt, loss_fn, dataloader, tokens2dims):
    &quot;&quot;&quot;
    &quot;&quot;&quot;
    t2d = np.array(tokens2dims)
    pred_dims = np.concatenate(([0],np.cumsum(t2d[:,0])))
    model.train()
    total_loss = 0

    for batch in dataloader:
        y = batch
        y = torch.tensor(y).to(device)

        # Now we shift the tgt by one so with the &lt;SOS&gt; we predict the token at pos 1
        y_input = y[:,:-1,:]
        y_expected = y[:,1:,:]

        # Get mask to mask out the next words
        sequence_length = y_input.size(1)
        tgt_mask = model.get_tgt_mask(sequence_length).to(device)

        # Standard training except we pass in y_input and tgt_mask
        pred = model(y_input, tgt_mask)

        # Permute pred to have batch size first again, that is batch / logits / sequence
        pred = pred.permute(1, 2, 0)
        loss = 0
        for k in range(11):
            # batch / logits / sequence ----- batch / sequence / tokens
            loss += loss_fn(pred[:,pred_dims[k]:pred_dims[k+1],:], y_expected[:,:,k])

        opt.zero_grad()
        loss.backward()
        opt.step()

        total_loss += loss.detach().item()

    return total_loss / len(dataloader)
</pre></div>
</div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[19]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def fit(model, opt, loss_fn, train_dataloader, t2d, epochs):
    &quot;&quot;&quot;
    &quot;&quot;&quot;
    train_loss_list = []

    print(&quot;Training and validating model&quot;)
    for epoch in range(epochs):
        print(&quot;-&quot;*25, f&quot;Epoch {epoch + 1}&quot;,&quot;-&quot;*25)

        train_loss = train_loop(model, opt, loss_fn, train_dataloader, t2d)
        train_loss_list += [train_loss]



        print(f&quot;Training loss: {train_loss:.4f}&quot;)
        print()

    return train_loss_list
</pre></div>
</div>
</div>
<section id="Initialize-Model">
<h3>Initialize Model<a class="headerlink" href="#Initialize-Model" title="Link to this heading">¶</a></h3>
<p>The model figure above sets a clear direction for the input and output dimension of the model. So now we just need to initialize it likewise. The core model we use is a basic Transformer, that is already implemented in Pytorch. The Transformer model is not the focus of this tutorial so therefore we load it from a helper file.</p>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[20]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>tokens2dims = [
            (4, 8),
            (4, 8),
            (4, 8),
            (4, 4),
            (4, 4),
            (4, 4),
            (4, 4),
            (8, 16),
            (10, 12),
            (10, 12),
            (4, 4)
        ]

model_spec = dict(
    tokens2dims=tokens2dims,
    num_heads=6,
    num_decoder_layers=6,
    dropout_p=0.1
)

device = &quot;cuda&quot; if torch.cuda.is_available() else &quot;cpu&quot;
model = Transformer(
    tokens2dims = model_spec[&quot;tokens2dims&quot;],
    MultiEmbedding = MultiEmbedding,
    num_heads = model_spec[&quot;num_heads&quot;],
    num_decoder_layers = model_spec[&quot;num_decoder_layers&quot;],
    dropout_p = model_spec[&quot;dropout_p&quot;],
).to(device)

opt = torch.optim.Adam(model.parameters(), lr=0.002)
loss_fn = nn.CrossEntropyLoss()

pytorch_total_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
print(&quot;Number of parameters in model: &quot;, pytorch_total_params)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Number of parameters in model:  264700
</pre></div></div>
</div>
</section>
</section>
<section id="6.-LOOP-(only-call-to-retrain-/-finetrain)">
<h2>6. LOOP (only call to retrain / finetrain)<a class="headerlink" href="#6.-LOOP-(only-call-to-retrain-/-finetrain)" title="Link to this heading">¶</a></h2>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>&quot;&quot;&quot;
EPOCH = 1
PATH = &quot;modelname.pt&quot;
LOSS = train_loss_list[-1]
MODEL_SPEC = model_spec

torch.save({
            &#39;epoch&#39;: EPOCH,
           &#39;model_state_dict&#39;: model.state_dict(),
            &#39;optimizer_state_dict&#39;: opt.state_dict(),
            &#39;loss&#39;: LOSS,
            &#39;model_spec&#39;: MODEL_SPEC,
            }, PATH)
&quot;&quot;&quot;
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[21]:
</pre></div>
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
&#39;\nEPOCH = 1\nPATH = &#34;modelname.pt&#34;\nLOSS = train_loss_list[-1]\nMODEL_SPEC = model_spec\n\ntorch.save({\n            \&#39;epoch\&#39;: EPOCH,\n           \&#39;model_state_dict\&#39;: model.state_dict(),\n            \&#39;optimizer_state_dict\&#39;: opt.state_dict(),\n            \&#39;loss\&#39;: LOSS,\n            \&#39;model_spec\&#39;: MODEL_SPEC,\n            }, PATH)\n&#39;
</pre></div></div>
</div>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[22]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>if IN_COLAB:
    PATH = os.path.join(&quot;partitura_tutorial/notebooks/04_generation&quot;, &quot;Drum_Transformer_Checkpoint.pt&quot;)
else:
    PATH = &quot;Drum_Transformer_Checkpoint.pt&quot;

checkpoint = torch.load(PATH, map_location=torch.device(device))
model.load_state_dict(checkpoint[&#39;model_state_dict&#39;])
opt.load_state_dict(checkpoint[&#39;optimizer_state_dict&#39;])
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[23]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>train_loss_list = fit(model, opt, loss_fn, train_dataloader, tokens2dims, 1)
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt empty docutils container">
</div>
<div class="output_area docutils container">
<div class="highlight"><pre>
Training and validating model
------------------------- Epoch 1 -------------------------
Training loss: 3.6172

</pre></div></div>
</div>
</section>
<section id="7.-SAMPLING">
<h2>7. SAMPLING<a class="headerlink" href="#7.-SAMPLING" title="Link to this heading">¶</a></h2>
<p>To generate a drum beat we need to call sample_loop from the model. The sampling loop will start with a <em>START</em> token in all fields (Timing, tempo, etc.) and autoregressively predict one note at the time until having a pre-specified number of notes.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[24]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>X = sample_loop(model, tokens2dims, device)
</pre></div>
</div>
</div>
<div class="nbinput docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>df = pd.DataFrame(X.cpu().numpy()[0,1:,:],
                  columns=[&#39;Time Half Note&#39;,
                           &#39;Time Quarter Note&#39;,
                           &#39;Time 8th Note&#39;,
                           &#39;Time 16th Note&#39;,
                           &#39;Time 32nd Note&#39;,
                           &#39;Time 64th Note&#39;,
                           &#39;Time 128th Note&#39;,
                           &#39;Pitch / Instrument&#39;,
                           &#39;Velocity&#39;,
                           &#39;Tempo&#39;,
                           &#39;Beat type&#39;])
df
</pre></div>
</div>
</div>
<div class="nboutput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[25]:
</pre></div>
</div>
<div class="output_area rendered_html docutils container">
<div>
<style scoped>
    .dataframe tbody tr th:only-of-type {
        vertical-align: middle;
    }

    .dataframe tbody tr th {
        vertical-align: top;
    }

    .dataframe thead th {
        text-align: right;
    }
</style>
<table border="1" class="dataframe">
  <thead>
    <tr style="text-align: right;">
      <th></th>
      <th>Time Half Note</th>
      <th>Time Quarter Note</th>
      <th>Time 8th Note</th>
      <th>Time 16th Note</th>
      <th>Time 32nd Note</th>
      <th>Time 64th Note</th>
      <th>Time 128th Note</th>
      <th>Pitch / Instrument</th>
      <th>Velocity</th>
      <th>Tempo</th>
      <th>Beat type</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <th>0</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>4</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>1</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>5</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>2</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>3</td>
      <td>4</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>3</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>6</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>4</th>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>3</td>
      <td>5</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>5</th>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>7</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>6</th>
      <td>1</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>5</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>7</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>0</td>
      <td>0</td>
      <td>1</td>
      <td>4</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>8</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>0</td>
      <td>4</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>9</th>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>1</td>
      <td>7</td>
      <td>3</td>
      <td>0</td>
    </tr>
    <tr>
      <th>10</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>11</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>12</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>13</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>14</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>15</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>16</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>17</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>18</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>19</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>20</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>21</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>22</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>23</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>24</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>25</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>26</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>27</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>28</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>29</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
    <tr>
      <th>30</th>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>3</td>
      <td>7</td>
      <td>9</td>
      <td>9</td>
      <td>3</td>
    </tr>
  </tbody>
</table>
</div></div>
</div>
<p>We can see that the sample loop predicts 30 notes (apart from the <em>START</em> token) but only 16 actual MIDI notes where predicted and from line 16 and down we get only the <em>END</em> token.</p>
<p>Finally, we can save generated MIDI files from the sampling process.</p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[26]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>for k in range(16):
    XX = X.cpu().numpy()[k,1:,:]
    na = tokens_2_notearray(XX)
    save_notearray_2_midifile(na, k, fn = &quot;PartituraTutorialBeats&quot;)
</pre></div>
</div>
</div>
</section>
<section id="Conclusion">
<h2>Conclusion<a class="headerlink" href="#Conclusion" title="Link to this heading">¶</a></h2>
<p>This is the end of the Tutorial for drum beat generation using partitura.</p>
<p>To play the MIDI files you can import them in your DAW of preference and load a drum VSTi or create your own DrumSampler.</p>
<p>In this tutorial, we learned how to create MIDI drum generation using partitura and a Transformer autoregressive model. We investigated some possibilities for encoding and tokenization of timing, instrument, dynamics, and tempo.</p>
<p><a class="reference external" href="https://colab.research.google.com/github/CPJKU/partitura_tutorial/blob/main/notebooks/04_generation/Drum_Generation_Transformer.ipynb"><img alt="Open in Colab" src="%22https://colab.research.google.com/assets/colab-badge.svg%22" /></a></p>
<div class="nbinput nblast docutils container">
<div class="prompt highlight-none notranslate"><div class="highlight"><pre><span></span>[ ]:
</pre></div>
</div>
<div class="input_area highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>
</pre></div>
</div>
</div>
</section>
</section>


        </div>
      </div>

      <div id="side-menu-container">

        <div id="search" role="search">
        <form id="rtd-search-form" class="wy-form" action="../../search.html" method="get">
            <input type="text" name="q" placeholder="Search..." />
            <input type="hidden" name="check_keywords" value="yes" />
            <input type="hidden" name="area" value="default" />
        </form>
</div>

        <div id="side-menu" role="navigation">

          
  
    
  
  
    <ul class="current">
<li class="toctree-l1"><a class="reference internal" href="../01_introduction/Partitura_tutorial.html">An Introduction to Symbolic Music Processing with Partitura</a></li>
<li class="toctree-l1"><a class="reference internal" href="../02_alignment/Symbolic_Music_Alignment.html">Symbolic Music Alignment</a></li>
<li class="toctree-l1"><a class="reference internal" href="../03_mlflow/pitch_spelling.html">Pitch Spelling with Partitura</a></li>
<li class="toctree-l1 current"><a class="current reference internal" href="#">Drum Generation Transformer</a></li>
</ul>

  


        </div>

        

      </div>

    </div>

<footer>
    <div id="footer-info">
        <ul id="build-details">
            
                <li class="footer-element">
                    
                        <a href="../../_sources/notebooks/04_generation/Drum_Generation_Transformer.ipynb.txt" rel="nofollow"> source</a>
                    
                </li>
            

            

            
        </ul>

        
            <div id="copyright">
                &copy; 2022, CP JKU
            </div>
        

        <div id="credit">
            created with <a href="http://sphinx-doc.org/">Sphinx</a> and <a href="https://github.com/Autophagy/insegel">Insegel</a>

        </div>
    </div>

    <a id="menu-toggle" class="fa fa-bars" aria-hidden="true"></a>

    <script type="text/javascript">
      $("#menu-toggle").click(function() {
        $("#menu-toggle").toggleClass("toggled");
        $("#side-menu-container").slideToggle(300);
      });
    </script>

</footer> 

</div>

</body>
</html>